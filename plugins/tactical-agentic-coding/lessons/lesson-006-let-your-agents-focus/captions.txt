# Lesson 6: Let Your Agents Focus - Video Captions

Source: Tactical Agentic Coding Course by IndyDevDan
Duration: 57:17

---

Welcome to lesson six of Tactical Agentic Coding. your first of three advanced lessons. In this lesson, we're focused on the last two steps of the software developer lifecycle, review and document. With these two steps, we nearly arrive at the future where you can run a single prompt that triggers a prompt that triggers a fleet of agents that ship your work end to end. We're not just talking about any agents though. We're using Claude Code agents that run your AI developer workflows to ship with your expertise in your product to solve your domain specific problems. With just the three steps we've covered, you will outperform any cloud-based agentic coding tool, but we're not done yet. After you complete this lesson, not only will you know how to plan, build, and test autonomously, build, and test autonomously, your agents will be able to review their work to make sure what they've done is exactly what you've asked. If the work doesn't align with your plan, they'll agentically fix the issue. If it does, they'll give you a concise summary of the work just like you or a team member would to concretely communicate what's been done and how it works. I've distinguished this step in the software developer lifecycle for this reason. For Agentic Coding, this is massively important for increasing massively important for increasing your review velocity. After review, your agents will then take a critical, often neglected final step. They'll document their work. This creates a complete software developer lifecycle feedback loop that makes agents you run in the future more performant because they know exactly what came before and when to include a given piece of documentation. So you might be thinking, what's the difference between testing and reviewing? Why testing and reviewing? Why do we have this extra review step. Some engineering teams group these together. I think that's a massive mistake, especially in the age of agents. Here's why. Every step of the software developer lifecycle can be represented as a question and an answer. For instance, the plan step asks, what are we building? The build step asks, did we make it real? Testing says, does it work? The review step asks, is what we built what we planned. And then of course, document asks, of course, document asks, how does it work? Notice testing answers the question, does it work? But review answers the question, is what we built what we asked for? So by reviewing, we're not talking about code quality or implementation details. We're handing all that off to our agents. We're asking a specific set of agents trained to review this question is what was built what we asked for now prove it the question and the answer is essential this is key for increasing is key for increasing your review velocity you may have already run into the constraints of Agentic Coding planning and reviewing here in TAC six we're going to improve on the review constraint you'll see exactly what that looks like in a moment these two final steps can drastically increase your agent to coding KPIs. We want attempts down, streak up, size up, presence down. So after we know our application works, thanks to our testing AI developer workflow, we covered in our previous lesson, lesson five, and we know the application we know the application contains work we asked for, thanks to the review AI developer workflow we'll explore in this lesson, we can then Document the work that was done. Documentation is simple, but how you document and when to include your documentation is the tricky part we'll solve in this lesson. Why do we document? Because inside the new agentic layer of your codebase, documentation provides feedback on work done for future agents to reference in their work. They can operate and then can operate and then update the documentation when the time is right. Great. So we know what we're doing, but what's the tactical advantage we're learning in this lesson. This lessons tactic, solves many issues engineers face when working with agents today this tactic drives great agentic development at scale because it forces us to make a controversial decision and commit to it in return we completely sidestep a massive list of Agentic Coding potholes and Agentic Coding potholes and problems the tactic is one agent one prompt one purpose This unlocks massive Agentic Coding capabilities throughout the new agentic layer of your code base. Why is that? Isn't more specific context better for your agent, the more it knows the better it performs? This is not true. Massive context windows often leads to a distracted, confused, agent. This is context pollution. This is context overloading. This is toxic context, wherever you want to call it. want to call it. When you overload the context window, your agent has a harder time focusing on what matters the most, the original task you asked it to complete. And yes, generative AI companies, big tech, they're chasing the all-in-one, god model the super agent that can do it all but that's not what we're doing here we're solving real engineering problems with our boots on the ground with real work ahead of us there's a reason why they're called ai developer workflows because they add compute to your developer workflows to your developer workflows the workflows you take every day as an engineer you and i as engineers we operate one step at a time and every step of engineering requires a different set of information a different approach a different perspective it requires a different set of tools and context okay think of a large feature you recently shipped even with in-loop agentic coding it doesn't happen in one swing there are steps there's process there's planning building testing reviewing and documenting all reviewing and documenting all right so we're breaking these down step by step. This is the power of the AI developer workflow. You get to now encode this, right? You get to template your engineering and hand off the work to your agents. You can teach your agents how to operate your codebase to solve your problems. Real engineering requires real developer workflows, not some pie in the sky God model that claims to do it all. And, you know, let me just spoil it for you. They won't, it's not here. It's not coming soon. The real engineering soon. The real engineering still must happen. One agent, one prompt, one purpose. You want to use specialized agents with focus prompts to achieve a single purpose. You've already seen this in the TAC codebases. No agent runs more than a single prompt. Now this doesn't mean the prompt isn't large or the prompt doesn't do a lot of work, right? You've seen this in lesson three, four, and five. We have meta prompts. We have templates. We have higher order prompts. These are not simple ideas. We're not doing simple sets of work. We're not sparing our agent. or the language model or the language model from complexity. What we're doing has many advantages. Let's run through the advantages of specialized one prompt agents. Why is this so powerful? You free up the context window. You give your agent the full 200K, 500K, 1 million tokens, whatever you're working with, whatever Agentic Coding tool you have, and whatever time you're in, you give your agent all of the space and the opportunity to solve the one problem well. This is important as your codebase grows. To execute work, your agent will likely need agent will likely need to pull in more and more context. As agentic engineers, we have three constraints, the context window, the complexity of our codebase, the problem we're solving and lastly our abilities specialized agents bypass two out of three of these all right model intelligence is not a constraint don't use this as an excuse it will set you back this is a losing mindset okay when you free up the context window It means you can also solve bigger problems in your workflows. And it also simplifies your it also simplifies your workflows. We work one step at a time, one agent at a time, one prompt, one purpose at a time. All right. The compact command inside of Claude Code, maybe this will be resolved in the future, but this is a bandaid fix. If your agent is running compact, it is losing information. Okay. So what's another key advantage of this tactic? One agent, one prompt, one purpose. It lets your agent, agent focus. This is a very similar advantage, right? This should be relatable for you as an engineer. A focused an engineer. A focused engineer working on a single task is a productive engineer. Agents are the same. Big context windows cause context confusion. This drops your agent's performance. And when your agent's performance drops, your performance drops. So let your agent's Focus. Every piece of context you add increases the number of variables your agent has to reason about. You want to context engineer as little as possible. You want the minimum context in your prompt required to solve the problem. When you adopt your agent's perspective, it should be clear it should be clear what context it has, the available tools, and how the problem can be solved with the focused leverage points it has available. Last but not least, one agent, one prompt, one purpose has a very special side effect. You likely already know what this is. You're seeing it in this codebase. We get to commit every one of our prompts, all of our workflows, which means we can easily improve them, okay? Since you're not stacking up many prompt calls from who knows where, we can easily reproduce and more importantly, reproduce and more importantly, improve every single step down to the prompt level. This is big. While other engineers and companies are running big context windows that'll be blown away at the end. You'll be running single focus prompts that you can pinpoint directly and improve. You've seen this in the code base. We just search for the slash command, open up the file, and we know what our agent can see, all right? When you're running in loop, right, hands on the keyboard, you can quickly iterate and run your prompts so that they're battle tested to run out tested to run out the loop with your AFK agent system. By doing all this, you effectively create evals for the agentic layer of your codebase. Whoa, whoa, whoa. Evals for your agents in your codebase. Yes, this is a big idea. We're not gonna explore it much more here in TAC. We have other priorities that are more important, but individualized agents, specific prompts, one purpose, you can evaluate that. Okay, you can change the model. You can add thinking mode. You can change your Agentic Coding tool. You can rerun these over can rerun these over and over. You can get checkout before you ran work and run it. right, you can literally create evals for your codebase, which means you accelerate on the path to improving your agentic KPIs that much further. And all that aside, you will passively, you will start forming evals in your head just passively as you build up the agentic layer of your codebase. You'll understand what works well and what doesn't work. So as we work through the review and document steps in this code base and the tactics codebase, notice how codebase, notice how we're using this tactic throughout the codebase. One agent, one prompt, one purpose, one, one, one. Let's dive into the codebase to see how we can build dedicated agents to review and document agentically. All right, so let's open up the terminal. Business as usual here. If we run LS, we can see the previous five codebases. Let's clone in the Lesson 6 repository, link available in your loot box below. I'm gonna CDN. and we're gonna boot up your favorite editor. I'm gonna use code here. So right away, we're going to boot up Claude. I'm boot up Claude. I'm going to once again use Yolo mode and I'm gonna fire up the Opus model here. Just as before, we can run our all-in-one set of command slash install. This is a great way to encode your engineering workflows for you and your team. If we open up slash install, we have one addition here at the bottom. we are going to set up some Cloudflare environment variables so that we can upload some image assets. This is gonna be really important for our review workflow. This is a little teaser of what's coming next. Aside what's coming next. Aside from that, it's the exact same workflow. We're gonna copy in some variables, reset the database and boot up our server in the background. Our agent is setting everything up for us. There is no reason to do this work by hand. You have agents use them. So a couple things right away while this is running in the background, let's go ahead and take a look at our new AI developer workflows. This is the new agentic layer around your codebase. And in order for this to work, it composes your prompts, okay? But your workflows detail exactly what the capabilities are what the capabilities are for your agents. Let's look at our new low-level ADWs that can be chained together. We have a brand new review. We have a brand new patch. and we have a document and you can see we have some composed methods that put them together. So we're going to be running plan, build, review and plan, build, document. We also might just run the document step on its own after we fire off plan, build, review. What does this do? What are we gonna be looking at here, right? So all of these composed workflows, they do exactly what you do exactly what you think they do. They run the lower level ADW, plan, build, review. And so if we just look through these, right, we look at ADW, we can see exactly what's happening here, right? We have plan running in a UV single file script build and review. It's really important that these are standalone scripts. We can then compose them and make them run side by side together. And the ADW ID is the thing that changed together the state of all of their work. So this is plan, build, review. All the other composite workflows, the other composite workflows, the composed workflows look the exact same. In plan, build, document, we have that exact same thing, ADW underscore, and then we have plan, build, document. Okay? It's important to note that you need something to test document and review against. So plan and build are our essential methods. It's the work that allows the other three steps to even exist. Checker agent looks like it's setting up our database. Great. And there's going to start the server in the background. That looks great. We've covered plan and build in previous lessons. Let's in previous lessons. Let's go ahead and jump right in to the review step. Open up our review AI developer workflow and let's understand what's going on here. So there's no excuse for having poor documentation. So at the top of this single file script, we have clear, concise, documentation. What's the workflow? Find the spec from the current branch review implementation against the spec. Right. So we need a plan and we need a diff capture screenshots of critical functionality and then resolve the issues if there are issues. So if what was built is what was built is not what we asked for, resolve the issues and make up the difference, post the results and push and update the PR. OK, so that's what this review does. So without even looking at the code, right, without even looking at the steps, we know exactly what this does. Let's see if our codebase is set up. Fantastic. It is. You can always review the required steps. So you can see we have some manual steps here. I'm going to work through these quickly, get this all filled out, set the origin back to the TAC six codebase. You're going to want to, of course, fork, set it to a new codebase. So you have access to GitHub issues and get GitHub issues and get up PRS set remote paste, skip push, filled out some of these missing variables. So I'm going to go ahead and say start application again, already just handing off work. to our agent so it can do it. There's no reason for us to do this anymore. And so if we refresh, you can see our application is in a great state. Select five users, fire it off. And we'll of course get five users here. Fantastic. So our application here is back up and running. Let's go ahead and create a new GitHub issue. This is our prompt source. is our prompt source. So let's go ahead and create an issue here and fire off a new workflow. Create issue. We want a new feature for our natural language SQL interface. Let's use something a little bit more interesting here. So one click, table exports. I'm just going to paste this prompt in here using the ADW workflow. Add one click table exports and one click result export feature to get results as CSV files. We want two new endpoints. I'm specifying where we want this to be triggered. Then I'm saying, you know, use the right download icon. So this is a high level prompt that's high level prompt that's moving into mid-level territory. We do have some engineering details here. Feel free to add as much detail to communicate exactly what you want done. You want these to mostly be high level prompts because we've templated our engineering already inside of our planning prompts, right? Our slash tour slash bug slash feature slash whatever class of problem you want to solve. So I'm going to go ahead and create this. And now we have an issue ID that we can use to kick off our workflow. As you can see here, I don't have my dedicated agent environment. We're going to use my going to use my device. We're going to let our agents run on this device to get work done so that we can analyze and understand what's going on under the hood in real time. So in a new terminal here, we can run UV run plan, build, review. I'm gonna copy the relative path to this paste and let's get our issue number. We have issue 14 here and we don't need an ADW ID since this is a new workflow. One will be created for us. That should be good. So we're gonna fire this off. Now our workflow is running. So our agents have now our agents have now taken over this repository. We shouldn't be doing anything alongside them. They are running their own workflows designed to ship end to end. You know, we're running plan, build, review while this is running. Let's understand the review workflow at a deeper level so that we know exactly what's going on here. Whenever you want to understand one of these workflows, go right to the main method. Let's run this top to bottom and just focus on the key pieces of information. All right. So we're passing in arguments. We're setting up our state. We have to make sure we have the right state have the right state to run this workflow. And if we open up the file explorer here, collapse everything and open up agents, you can see we have this ADW ID with every dedicated agent, right? All their logs is getting placed here. And we have something really important. We have state meta information that travels along every one of our AI developer workflows. It's really simple. As you can see here, ADW ID key fields, branch name, plan file, issue class. Okay, our workflows validate that the state that they need exists from the state exists from the state file, So, you know, whenever you see state that is exactly what's happening here. We're checking our environment variables, getting the GitHub repo from the state object and then we're validating, right? You can't review if you don't have work to review and you do work on new branches. So you can see here, we're just validating the branch, making sure we have that checked out. And then a lot of all the code here is just logging and communicating to our Peter review system. Remember the Peter framework, we have prompt input, trigger environment and our review system. Our our review system. Our review system is of course GitHub issues here. If we scroll down, you can see we're writing updates. Right now our agents are working on the implementation plan. We find the spec file again, coming out of the state. We log and then here we have something really, really cool. We are initializing our R2 uploader. So we're going to upload images based on our review to a Cloudflare R2 bucket. This of course can be any bucket you can imagine. We just need this for publicly hosted images so that we can attach them on our GitHub issues or on the PR, wherever you'd like to place you'd like to place them. And then we have the crux of everything. Okay, so review runs two workflows. You know, we have two big steps in this workflow. We run a review and then if there are issues, right, blocking issues, we then attempt to fix and remedy the issues agentically, right? You don't want your agents bothering you. You want to reduce the number of human in the loop points there are in your workflow. As we know, tools will improve, models will improve. This is going to keep happening. You want to prepare for the future. And of course, we have tools to come into tools to come into this workflow and to understand what went wrong if we need to and to apply patch fixes, which is what the ADW patch is for. Okay, but always lean toward the future. We're not here to stay in the loop forever, right? They're called Outloop systems for a reason. You want your agent shipping end to end. And if something goes wrong, you fix the end to end portion, not the thing that went wrong, right? This is the tricky part. And this is the important transition we and I have to make as agentic engineers. Okay, but let's look into the run review and guess what it's doing. It's really simple. doing. It's really simple. It is running the review prompt. You know, you can always just search and understand where all the prompts are getting fired off. You know, search all slash command equals template request, execute template, right? It's all there in the codebase. Very easy to find. Let's look at the review prompt. Everything comes down to the prompt level. All the ADW surrounding deterministic code is just support for the agents doing the real work. So what does review do? Of course, you know, we have a, you know, growing library of prompts that drive our agents behavior. And in agents behavior. And in the review prompt, we're doing a couple essential things. If we just collapse everything, we can see a simple view of this prompt and understand it at a high level. There's always a great place to start when you're reviewing prompts, collapse, keep it simple. Don't get overwhelmed. Follow the instructions, review the work done against a specification file, right? Use git diff, understand the changes that were made, capture screenshots and report any issues. Okay. If not report success. Great. What are we reporting exactly? You can see our variables here, ADW spec file, agent name, review image directory. name, review image directory. So we're being really clear, right? You can just create variables at the top of your prompts that'll then get used as long as you're referencing them. What are we doing here? information dense keywords on the fly. These are variables that can be used and are used throughout the prompt, right? If we search for ADWID, you can see it getting used there. If we search for agent name, of course, you can see that there. And of course, our review image directory, this is getting referenced directly in our instructions. What does the output format look like? Of format look like? Of course, you can look at the instructions there. Here's the report, right? This is the interesting part. We are outputting JSON from our review prompt. Of course, you want to leverage types. This is a key leverage point of Agentic coding. So everything we're doing here has a corresponding type. If we just search for review result, you can see we have this type exactly, right? Success, review summary issues, screenshots, screenshot URL, yada, yada, yada, right? And it contains, of course, a nested type review issue. The important thing to call out here is that out here is that in a review issue, we're looking for three classes of issue. Okay. We have skippable tech debt and blocker. You want to make sure you're accounting for things that your agents are going to find in your work when they're reviewing your code and your application. Okay. So we don't want to block on skippable or tech debt. We want to block on blockers. Okay. So we're communicating this to our agents. They will find something if you ask them to. All right, these agents are very agreeable, but they're also just great at engineering. They're great at engineering. They're great at coding, they're great at debugging. So if you ask them to find something, they'll find something. You wanna give them the tools to delineate what is important. Okay, once again, we're templating our engineering into our prompts for agents. We're adopting our agent's perspective so that they know what they need to do at the right time. Okay, so that's that, and this is the review prompt. So it's going to look through the application. It's going to review the code done. check it against the spec and return this object structure. And so that's what we get here. That's the response here. That's the response structure. And then at some point here, we're gonna parse it. Yep, there it is, parseJSON. There's the Pydantic type. And then we're just gonna return that, of course. Great, so that's review. We're gonna upload and map the screenshots. Nothing new there, we're uploading to a bucket. And let's see how far our agent is here. Build phase, okay, so our agent is in the build phase, that's great. We can always check the agent ADW ID directory, right, agent slash ADW ID here, and understand where they are in their process. ADW review doesn't exist yet, so they're exist yet, so they're still on the build phase. Great, that gives us a little bit more time here to discuss the resolution, okay? So we have a max loop here, so we're not gonna keep fixing issues if our agents just can't if they can't of course they're going to let us know right and then we will have to hop back in the loop and fix our agentic layer to resolve the class of issue that caused the review retry attempts to run out we're going to run review and if there are issues right so if we have issues here we are going to scroll down and we're going down and we're going to resolve review issues so this is super powerful if we hop in here you can see we're going to loop over all the blocking issues, right? Specifically the blocking issues. We're gonna create agent names to be passed in to a reusable method, create and implement patch. If we hop into this, we have a shared method in workflow ops, and you can of course see where that's stored here, ADW modules. Inside of this, create and implement patch, we are going to create a patch plan. And so the patch plan looks like this. plan looks like this. If we open up patch, you can see, this right here, right? Always collapse and just look at a high level, create a focus to patch plan, resolve a specific issue. So we're making a surgical change here. Follow the instructions to create a concise plan to address the issue with minimal targeted changes. And so this is a really powerful prompt. Of course, we can't just do everything with our ADW first shot. We're going to need to come in and improve things to patch issues. So we have a concise patch plan. This is you know, to be super clear, very similar super clear, very similar to our chore prompt and our bug and our feature prompt as well, right? So these are a template meta prompts. They're going to output a plan that we can then execute after doing research. So these are all dedicated prompts that run single agents that do one thing well, right? They have a single purpose. Okay. And so you could very easily see when you're writing these small patches, that's very different than a chore, a bug, or a feature, right? We're just changing one thing. We changing one thing. We want a simple quick fix. This is how we do it. Just to mention it here, ADW patch does exactly this, okay? It runs this workflow, right? So create and implement patch. And so the first thing we do is create a patch plan, and then we run our implement plan. And we all know what implement does. It runs the slash implement agentic prompt. We can open that up, of course, as well. And you'll see we are starting to reuse our prompts. This is a higher order prompt. It takes in a plan, right? in a plan, right? A path to a plan. We're going to pass it in. And just like our plan from chore bug or feature is going to do that exact same workflow. It's going to read the plan, think hard and implement. Okay, so that's it. So you can see we're starting to get some reuse of the compositional pieces of our code base. This is really important. And again, we're hitting on that theme of reusability and single agents with one prompt with a single purpose. Okay, so this is powerful after the implement powerful after the implement plan runs through. Let's close all these. We go back up and we have the patch file and the implementation response. And of course, we're just doing additional logging, looking for success, looking for failures. And if there's a failure, we try again. Okay, so that's what ADW review does. Let's check in with our workflow. It looks like we are still building. That looks great. And whenever you need to, you know, you can hop up to the issue or whatever your prompt review system is, and you can just see what's going on, right? So you can see we are still implementing we are still implementing the solution. Having a review system is really important. As we discussed in lesson five, you want to be operating out the loop over the long term. As you improve here as an agentic engineer, it will become better and better and better at operating the codebase without you because you've put in the work into the agentic layer covering the software developer lifecycle. So you can see here, it took about eight minutes for our agents to implement the solution. We should have a commit and a PR following here pretty soon. This will update the existing PR, update the existing PR, which right now will just contain the plan for this, right? Thanks to the plan step in our software developer lifecycle, AI developer workflow. Review issue, come back up to the top. Again, do a bunch of reporting a lot of this code just to make it super clear. It's just reporting, right? Look at all these hits on the sidebar here. This is just reporting. Most of this code is just communicating what was done. This is super important. You always want to be communicating the work that you've done to your review system so you can easily review. And just to mention And just to mention it again, you know, It doesn't matter what you use for your review system, right? Maybe you like GitHub issues. Maybe you want to put it on the pull requests itself. Maybe you want to use a entirely different resource, right? Jira. I can't imagine someone wants to use Jira, but you know, you might want to use something like some Jira, some type of task management board, whatever you want to do. do it your way. All we're doing here, I'm not showing you how to do it exactly. I'm showing you what you can do. Keep the Outloop system in mind and always know, of course, there are gonna be times when you need times when you need to come into the loop, work in the terminal, fire up instances that you're prompting back and forth. And it's great for proving your workflows and making sure that they work. So like for instance here, you know, in my install script, it looks like my agent kicked off start on its own, which locked up its loop. So this is something that we can improve in the install command, right? Make sure to be running on the background process. And to be fair, I probably should have just communicated that in the inline prompt. Anyway, so right now our agents are creating the implementation commit and you can always dial in can always dial in to the exact agent that's running. You can see it's prompt there and the raw output. This is from the stream call. when you're running Claude Code with the stream output mode. And it's just going to stream this. And then at the end, when it's finished, we format into a nicer JSON object. And so, you know, something else to note here, our agent is running this device. So you can see my branch has changed, right? Our agent is in full control over this system. I'm not going to be running anything right now because it has full because it has full control over this repository, right? If we want to, we can see its work, git diff dash dash stat. and we wanna run this against probably origin main. And so you can see all the changes it's made so far against the origin main branch. So you can see there's our plan. It created a nice test for us there, so on and so forth. It has our end-to-end tests, it's creating this. This is part of our engineering. We've encoded it into our template. So this stuff just happens all the just happens all the time now. right? We've templated our engineering and our agents know what to do with this stuff, right? We're preparing for, you know, that world where we are just staying out the loop. These are all tactics that you should be thinking as you're operating with your agents, right? You should be thinking these things, right? Templating my engineering so that I can stay out the loop, building systems that have one agent, one prompt with a single purpose. Okay. If we look at the agents, AWID directory, we can see that we are reviewing the code now. I'm really, really excited to show really excited to show you what's coming next. You can see here R2 upload enabled and we have this public bucket that we're going to have access to. And you can kind of imagine what's going to happen next, assuming that this workflow works as intended, assuming our agent ships the work that was designed to ship. You never know. Things can always go wrong. These are non-deterministic systems. But the goal is to make it so that we have an entire pipeline of agents that are checking each other's work, making sure things look good and communicating to us if that's not the case, and then we case, and then we can improve our composable units. And so this is the review step. We have, you know, kind of three distinct agents that are running here. It's a good practice to encode the agent names as constants right at the top of the file. So it's super clear what agents are running, what agents are controlling that given workflow. We can always just search one of these, you know, output logs, and we can see exactly where our agent is at in this workflow. So, you know, we are reviewing against the specification. Right. So super important point running the review, uploading images next. And uploading images next. And then we're going to resolve any blocking issues if any existed. Okay. It looks like our workflow has been completed. Plan build review. So up here we can see something incredible. Okay. We have images uploaded. And so we can click into this and we can see right on our public URL here, we have that down arrow. Okay. And so there it is there. It looks like it is off here. So our agent did not obey this properly, right? The export should not be in the center there. And we can run a patch can run a patch to clean this up. This is a perfect example of needing to have a concrete review step and improve the review step. So, you know, we can see that and our agent is reporting one click feature export has been implemented implementation, blah, blah, blah, blah, blah, error handling all tests passed. As you can see, here, there is work to be done on improving the prompt and the workflow for this review process, right? We have our export button right here and we asked for it to be right here. Okay. So let's go ahead, put in a patch for this. And, you know, of course we can, let's go ahead can, let's go ahead and open up the code as well. So I'm going to Huffactor agent here, I'm gonna type slash start and it's going to run a start command to agentically kick off the client servers. It's gonna kick it off again, there we go. And so you can see we have our download here. So if we click this, very cool. So if we click this and if we click download product, we should see the same thing, great. We have a download there, select five users, just running a random query here. And this is where our issue is, okay? So yeah, so this is right, the wrong formatting. This should be right next to the hide button. If we hide button. If we click this, you can see we do get the download which is great, but of course our UI is a little off. So this is a great example to showcase the patch prompt. We will run into issues. We need a mechanism inside of our workflows to quickly knock them out, ideally out the loop. Just real quick, I'm going to type ADW patch. I'm going to operate this like we're working outside the loop just for good practice. I'm going to copy the ADW ID, paste that there, and then we're going to say, what are we patching? make sure download button for sure download button for query results. And that should be it, right? So a simple patch, I'm gonna write a comment here and now I'm gonna run our patch workflow. Remember everything runs from the prompt source. So that's there, I'm gonna open up here and then let's go ahead and kick off this new workflow. This is gonna be UV run and now we're gonna run a W patch, copy, place that there. We need the issue ID, it's gonna be 15. And we also want the ADW ID, right? We're not starting new work, we're building off existing work. So I'm gonna paste this in. That's paste this in. That's the ADWID, you can see it found the state. Okay, so I passed in the wrong patch ID here. That's PR15, it's issue 14. It's gonna copy, paste, rerun against 14. There we go. I did find the state, which is good, but we need to run on issue 14. There it is. For query results, make sure the download button is just to the left of our hide button. So what's a potential reason why this happened? First off, I skipped the test workflow. Okay, so, you know, the UI test very likely would have caught this. Our review caught this. Our review is just looking to see if the feature is there. We always just have to be ready for a model just to blatantly make a mistake like this. And we look at this result, right? Like placing this here is definitely something our agent should have caught. How could we have improved this? There are a couple of places to tackle, right? we're running the plan, build and review. So we can really attack any one of these surface areas. The review step really should have caught that. Okay, so what's something we can do here in the review step? You know, something that You know, something that is just completely missing here. Our agent is capturing these images and something that we can add here that could be really, really important. The actual screenshots never make it into the agent. To be super clear here, our agent is operating Playwright. If you look at the root directory here, we have this .mcp.json and we have a simple Playwright configuration that allows our agents to operate Playwright. And so something that we missed here and something that isn't activated is we don't have this dash dash have this dash dash vision. We're not operating in vision mode. The tokens aren't flowing through the vision capabilities of our language model. So this is something we could add to tweak the workflow. I don't like this because this is outside of the ADW. So I want to kind of stay away from that and just keep general configuration here. Something we could add here is inside of the instructions for every image you take, be sure to read it in during the review process. Okay. And so This is something that we could very that we could very well add. And then our agent would likely read the image, have it in its context window, and then be able to improve on this small issue here, right? Because it was really just that one UI tweak that caused the issue, right? Let's see how our prompt patch is going, right? This is the whole point of the patch workflow. We have a patch plan created. This is getting created inside of our specs directory. And it looks like our fix was just finished. So our patch just completed. But you can see here inside of specs where we normally just have plans, we now have this patch directory. Okay. And patch directory. Okay. And so again, we are recording every large piece of work that occurs. The biggest piece of work is the actual implementation work. So we always want to have a plan that details exactly what gets done. So this is the output of our patch, right? So just to be super clear, this patch prompt, right? This patch agentic prompt creates this patch plan, super simple, super concise. And you know, you can see it has detailed the issue summary and the solution. Okay. Not correctly positioned, create a button container to group button container to group export and hide right side of the results header. Okay. And so we should be able to just, you know, refresh, select random product. And now we should see this. There we go. Export is now right next to hide. And you know, I know I absolutely can just hear a couple of engineers watching this now. Why didn't you just do that by hand? You're missing the point. Okay. The whole point here is that we are putting in the effort to build the system that builds the system. Yes. Like I know that you and I can use and I can use these gigantic tools. We can even do it by hand if we're being really dumb. We both know that we can fix this instantly with a prompt. Okay. That's not the point. This isn't about you anymore. Okay, this is about your agents. It's about teaching your agents how to build on your behalf, right? So that it can be about you again, okay? Our hands, our minds, remember tactic one, stop coding. That is going to start pushing into some of these hands-on pieces of engineering, like stop these one-off low-level prompts, okay? You really prompts, okay? You really wanna extend that as much as you can, right? Teach your agent how to do it. We can, of course, download this one row. Not a big deal there, right? Download, download. This all works great. We can of course open up. There is our products in a nice CSV file. The feature obviously worked. The important piece was implemented for us end to end. So this is great, right? This is the importance of having review. Now, you know, something really important that I wanna call out about this review process. Thanks for our Outloop system, we were able system, we were able to quickly identify this issue and just patch it, just right away patch it, okay? Now we are running in the loop, so we had to do a little bit of manual work, but out the loop, have to do is write, where was that patch at? Yeah, out the loop. I just write this and then the web hook triggers the Outloop system to just get the job done. Okay. So another reason you don't open up the environment, you don't do the stuff yourself, let your agents do the work for you. I know this is kind of controversial, but do you want to be an agentic engineer or do you want to be an engineer of the an engineer of the past? Engineers doing it the old way, hands-on, back and forth, back and forth, back and forth. Okay. Let yourself move up the stack. Okay. I know this is going to be a challenging thing to do, but it's critically important that you are a commander of compute, hand off more work to your agents, even small stuff like this. Don't bother yourself with this. Focus on the high level, let yourself scale up, use agents, use compute. Okay. So that's, that's reviewing. We can patch, we can get a full end to end review. And you know, this is something, this is a concrete improvement. I can concrete improvement. I can make to the review process, right? And this is something that you would want to do, right? For every image you take, be sure to read in, read it in during the review process. And then, you know, something like this, right? To make sure it matches the spec. and the original request. Question I can ask myself as I'm building the system that builds the system, do I have the original request? If you remember right at the top here, I have these instructions, right? Place a blank, blank, blank directly to the left of their respective thing, right? Direct to the left Direct to the left of the hide button. So this means I can ask myself, did I have the original request inside the plan when it got implemented? And so, you know, we can just look, right? We have all the artifacts we need, just let's just look, right? So where is that feature here? This is 14 and it looks like I did. So this was here and it looks like it is up to our review process to just be clear about this. Okay. And if we just search left here, this was there, right? So our agent just missed this, right? It just missed this It just missed this detail. And so this is something that is just going to happen, right? As agents improve, as tools improve, this will happen less, but this happened here. It's also really important to note. I don't wanna keep harping on this too long, but it's important to note that we did completely skip the test step, right? Which operates and validates end to end, right? So we skipped the test step. Our export functionality was not run, okay? And so you can see here, we do have that instruction. Our agent was very clear here. Make sure clear here. Make sure download appears to the left of the X icon, to the left of the hide button. So again, you wanna be really careful with the steps you have in your software developer Lifecycle, testing is there for a reason, right? This is a great showcase of, you know, testing very likely would have caught this issue, right? If we look at our ADWs here, remember we ran plan build review. We completely skipped plan build test review, right? We could have run this workflow here, right? This is here, right? This is probably the winning workflow. Let's move on to documentation. So these are all composable. So what we're gonna do here is just append document. We're gonna document this feature right at the end here, okay? So I'm just gonna open this up, UV run, paste the file path, of course, run against the issue ID, and I am going to run on the same workflow, right? The document step needs existing work. I'm gonna copy the ADW ID, paste that, and let it kick off, okay? So this is our documentation step. You can see there's You can see there's the output from the Git diff. It can see all the changes that we just made. And now it also has the spec file, right? The plan file. And now it's going to just document for us. You know, what does the document step do? What does document do? What advantage are we getting here with the documentation set? So documentation has two steps. So after your agents finish their work, they're of course going to create documentation. All right, so that's what this ADW document does. And of course we can just search slash document and we can find the prompt exactly. What the prompt exactly. What does this prompt do? Document.md. You know exactly where all these are. This runs the following instructions. So this is a workflow. And of course we can collapse, generate concise markdown documentation for the implemented feature. This command creates documentation in app docs. Okay, so we have this brand new directory here. This is part of the agentic layer of our codebase. If you want, you can also throw this in AI docs. I like to separate these out. AI docs for third party app docs is for your codebase run, get diff against run, get diff against main branch to understand the change. Right? So we're, we're using this as a pattern, right? You have the get diff, you have your plan. Now you can understand what was implemented, right? What was done. So, That's what we need for document. And of course we have this in the run up to the document workflow, right? So generate documentation. If we search this, we're back in main. And of course it's going to validate that we have a branch and this gives us something to work against, right? It gives us something to document against. So that's great. Documentation is pretty simple. Documentation is pretty simple. We just generate documentation and then we finalize the get operations. So this should, probably be done. Yep, there it is. We're already finished. And now we have documentation. So if we get diff again, and we run, what is it name? Now what's that command? It's get diff name only. Right. So that's it origin main. You can see we have all these files and importantly, we have app docs and we have this new piece of documentation. Okay. So we can open this up and we can see exactly what was created here. So it is important to note that just like our patch, plan, just like chore, just like like chore, just like feature, just like bug. Our document is a template meta prompt. Okay. So it has a document format that we specify. And here, of course, once again, you can template your engineering. How do you like to document, right? How do you like to keep track of things? I have a nice template here you can use to get started. Of course, copy, steal, make this your own, use this as a starting point. If we look here, you can see we have a, just a great concise breakdown of what was done. Okay. So if we just search for export, You know, you can see this all over the place. We over the place. We added export. If we go into markdown preview mode here, we're also going to get our screenshots. Okay. So there's a screenshot there and we didn't rerun the review step. So we have our previous export there. This is an improvement we want to make to our workflow. That's great. If we look through this, you can see exactly what was built, exactly what was changed. All the files here to support this changes, modified, clear, concise documentation, how to use it, so on and so forth. Right. and allows us to automate some of the automate some of the work that engineers just don't do or work that chews up a lot of engineering time, right? Agentic documentation, agentic review is ultra, ultra powerful. And now we have this piece of documentation, okay? So this is powerful, but this is not all that was done. Remember in the beginning, I mentioned that there are two steps to documentation, creating documentation for the work you've done and then there's knowing when and how to pull in the documentation. So if we open up the documentation prompt here, not only is it a template meta prompt, but template meta prompt, but it is also going to do something special. It's going to update a prompt, okay? So we're gonna update our conditional documentation. What is this exactly? What is conditional documentation? Let's go ahead and open this up. Let's take a look at this. So we have a prompt here, right? A lower level prompt that is used in high level prompts like the document prompt. And this is going to conditionally add documentation to the high level prompt, right? To the agent calling the high level prompt. So this prompt helps you this prompt helps you determine what documentation you should read based on specific changes you need to make in the codebase. Review the conditions below and read the relevant documentation before proceeding with your tasks. So it's quite simple, right? We're telling our agents, read this documentation given a condition. And so you can see here, our agent has agentically updated this file for all future agents referencing this file, right? And here comes the magical part with agentic documentation. If we search all for conditional docs, all for conditional docs, guess where this is gonna be? Guess where you're gonna find this? Inside bug, inside chore, inside feature, inside patch, and even inside our prime command, okay? This is very, very powerful. And if we go into, let's say we're fixing a bug, right? It's inside the relevant file section, right? Let's collapse everything else. We can get a high level overview, right? Our bug is our template meta prompt that is creating a plan to fix a bug. But if we look at the relevant files here, we're saying, read this we're saying, read this to check if your task requires additional documentation, okay? If your tasks matches any conditions listed, include those documentation files. in the template, right, in the prompt that this meta prompt will generate, all right? And so this creates a very, very powerful end-to-end full feedback cycle between the beginning of the software developer lifecycle with the planning and the end with the documenting, okay? So I hope you can see how this all changed together. this all changed together. This is a full, complete loop. Our agents are fully connected, right, from planning to building to testing to reviewing to documenting their work. It's all connected now. This seals the deal. This connects them all. So we have a powerful conditional doc, right? This is a simple pattern that advanced language models like Claude, like Sonnet, like Opus, like models of the future. They'll be able to support, you know, looking at this file, checking the conditions and then pulling in the file if needed, right? They won't just come in here and read them. And if they do And if they do scale up your model, right? Use something smarter. All right. This is super powerful. This is the conditional documentation. This gets updated. during the document step, right after the feature has been completed, after all of the steps of the software development lifecycle up to documentation. So we know the code works and we know that the code is what we asked it to be. What was built is what we asked for thanks to the review step. After that runs, we document. Okay. And so all the prompts are here with all of these steps together, with all this together, with all this work, right, where we have agentic documentation, all of this stuff is happening for us. If we open up the issue, you know, open up the PR, that's been created for us, that's been updated. You know, a couple of prompts, we have 900 lines of code of engineering work, but not just coding, right? You know, 80% of coding has been solved already by language models, right? AI coding is not enough. It's not just about updating code, right? Engineering is about much more. We can see this, we know this, and now we can solve for this with agents in our code base, right? With the base, right? With the agentic layer that we've built up with focus, one agent, one prompt, one purpose. You know, you saw end to end here, right? We now have assets. We now have documentation. We have a plan. We've patched some work that wasn't quite right. We have some improvements to make, you know, just to make it clear, running the ADW test, would have caught that. So we wrote a patch to resolve an issue. Our test, our plans, our end-to-end tests still got generated out of our plan step. This is super critical. And then we documented all the work that was done and we updated our conditional documentation, which tells all future agents inside of our bug, inside of our feature, right, patch tells all future agents running plans for new work, net new engineering work. It tells them what documentation they should read in. If we open up conditional docs here, it tells them when to pull in pieces of documentation. And so here we are, we have completed the software developer lifecycle. Back at developer lifecycle. Back at the issue, we have this really, really powerful feature where our agents are presenting the work to us. Okay, this is super, super critical for agenting engineering work. You don't want to be in the loop. You don't want to be on the ground and opening up the application. If you need to, of course you can, right? Go lower level when you need to, go into the application when you need to. But here we had our agents just present us with the proof that they have reviewed the system. Now, the review was wrong. They should have caught this. We did say this. We did say to the left of the hide button. So there are some improvements to make on our side, okay? I'm not blaming the model for this. I'm blaming my skills. I'm blaming the ADWs we have set up right now, okay? That's the problem here. But notice how quickly we were able to catch this with proof of value, right? Agentic proof of value. Review answers the critical question. Is what we built what we asked for? Even when your agent is wrong, when it presents you with proof that it thinks it's right, you can quickly say Yes or no. say Yes or no. Not all engineers are working on front-end systems. If you're building language models, data analytics work, building scikit models, old school scikit models, right? Classification models. Maybe you're doing some rapid prototyping, right? Whatever you would do to review that the thing is what you asked for, whatever steps you would take, this is what you're going to embed into your review step. This is what the review step does for you. Okay. This is the review step. This is ADW review. You want to encode for this specific codebase for the problem that you're solving. You want to solving. You want to encode how you would review to answer the question is what was built, what we asked for. Okay. And so you want some type of hard asset that proves that what was built is what you asked for. So it could be some type of report. It could be, you know, for UI, like we have here, it is a image. It also can be a video. You want to take this to the next step. What you want is a live asset that you can review that proves that what was built is what was asked for. And this is super, super critical because it directly increases because it directly increases your review velocity. And that means your agentic coding KPIs. And so I hope you can see that you want to put all these steps together. They're more powerful together. When we look ahead to what's next, really important to mention that the value in your ADWs isn't in a single workflow, right? It's the power of combining workflows, having them separate yet working together. You want them isolated. You want them reviewable. You want them improvable, right? And this is why we have is why we have one agent working on a single problem with a single purpose, right? One agent, one prompt, one purpose. What are the benefits of this? You free up the context window. You can let your agent focus on a single problem. Your prompts become improvable as well as reusable, invisible, of course. And lastly, you create a system where your code base can be easily evaluated, right, in a deep technical sense and just passively as you work and build the agentic layer of your codebase. So now we can agentically review and document. This completes and document. This completes the software developer lifecycle. So in our next lesson, we're going to put this stuff to work, right? I fully realized that we have been operating on this kind of simple toy example. That's fine, right? Obviously we're not focused on what is getting built. There isn't, enough time for me to cover all the different codebase types and codebase applications that every engineer watching this is operating in. There just isn't enough time. So this exactly doesn't matter. But what does matter here, and in our next lessons, we're gonna showcase this. What matters is the velocity you can achieve with you can achieve with an agentic layer of your codebase like this. I'm gonna show you how quickly you can really ship in our next lesson, okay? It's going to be absolutely mind blowing. I wanna really nail home the point that you can't compete with this, okay? So that's what we're gonna do in our next lesson. Every lesson, every tactic, every prompt we've run, in loop, out loop, it's all leading up to this big reveal of Tactical Agentic Coding, right? I'm gonna reveal the secret of Tactical Agentic coding. Understanding the secret is mission critical to is mission critical to your success in Tactical Agentic Coding. It's something that's been hiding in plain sight throughout TAC and it'll upscale everything we've done here in TAC. It's a big idea that you may have noticed that starts small and then scales up. If you're thinking ahead and you're applying these tactics already in your codebase, you've realized the secret already. With the five steps of the software developer lifecycle in TAC the agentic layer of our codebase, we can ship end to end, back to back to back, and we can continue to improve our agentic layer, right? We agentic layer, right? We can fix the small issue that we had with the positioning, right? We can encode that, we can template that into our engineering. All the work we're doing here is going to improve your Agentic Coding KPIs. This is how we know we're improving our Agentic Coding capabilities. This is what all these steps in the software developer lifecycle helps us achieve, right? That's why we built the agentic layer. We want attempts down, size up, streak up, presence down. As a concrete example of this, we ran of this, we ran one prompt at the beginning to add export functionality to our tables and our response queries. And we had an issue at the beginning of this execution, right? We just ran that one prompt. We had one attempt, right? This is the optimal state for your attempt KPI. We then ran the patch that moved our attempts up to two. This is not good. But on the bright side, the work that we did ship with that one prompt was solid, right? We were able to export CSV from our tables and from the query result. So size was good So size was good there. We did everything we wanted to in the right flow, right? We did break our streak because we needed to call two AI developer workflows to ship the feature. Okay. So our streak was broken, right? Reset back to one. Okay. And how is our presence? Obviously I'm operating here in the loop to showcase tactics of Agentic Coding, but how was our presence, right? Very quickly, we were able to identify an issue thanks to our review images and then quickly patch it. So our presence was something like three. We were there in the beginning. We saw this issue. We ran this issue. We ran the patch and then we reviewed it again. Ideally, you want two moments of presence, one at the beginning for your prompt input, one at the end for your review and your PR merge. That's the optimal case. As you'll see in our next lesson, you can actually push that further. This is where things are going to start to feel and get a little wacky as engineers. In addition to the secret of Agentic Coding, we're going to move even faster, just like you can with individual agent instances. You can run entire AI developer workflows in dangerous mode, workflows in dangerous mode, where we ship all the way to production. We're gonna showcase exactly what that looks like and how and when we would use this, because if you're on the edge of this stuff, I think you can start to see how and when we could use this, not for big features, right? Not for serious work like that, but for maybe some bugs, maybe some chores, we can start to run entire workflows in yellow mode but i'll see you in lesson seven where we dive in to the secret of Tactical Agentic Coding the secret of Agentic Coding great work here do not wait to start putting wait to start putting this stuff in your codebase set up basic adws set up basic agentic prompts get your advantage start rolling this into your code bases the value here is immense i'll see you in lesson seven we're going to reveal the secret of agentic engineering