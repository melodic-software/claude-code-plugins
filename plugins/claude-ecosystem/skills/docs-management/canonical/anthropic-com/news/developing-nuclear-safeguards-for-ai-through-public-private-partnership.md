---
source_url: https://www.anthropic.com/news/developing-nuclear-safeguards-for-ai-through-public-private-partnership
source_type: sitemap
content_hash: sha256:16f00ad7a3e9f68d6368e534be855fa31520ecf30f20d6e5feaee37b8880bd76
sitemap_url: https://www.anthropic.com/sitemap.xml
fetch_method: html
published_at: '2025-08-21'
---

Announcements

# Developing nuclear safeguards for AI through public-private partnership

Aug 21, 2025

[Read the full post on red.anthropic.com](https://red.anthropic.com/2025/nuclear-safeguards/)

![](https://www-cdn.anthropic.com/images/4zrzovbb/website/74409af25137110ac04cc39e4d5ea0a2fbcea421-1000x1000.svg)

Nuclear technology is inherently dual-use: the same physics principles that power nuclear reactors can be misused for weapons development. As AI models become more capable, we need to keep a close eye on whether they can provide users with dangerous technical knowledge in ways that could threaten national security.

Information relating to nuclear weapons is particularly sensitive, which makes evaluating these risks challenging for a private company acting alone. That’s why last April we [partnered](https://www.axios.com/2024/11/14/anthropic-claude-nuclear-information-safety) with the U.S. Department of Energy (DOE)’s National Nuclear Security Administration (NNSA) to assess our models for nuclear proliferation risks and continue to work with them on these evaluations.

Now, we’re going beyond assessing risk to build the tools needed to monitor for it. Together with the NNSA and DOE national laboratories, we have co-developed a classifier—an AI system that automatically categorizes content—that distinguishes between concerning and benign nuclear-related conversations with 96% accuracy in preliminary testing.

We have already deployed this classifier on Claude traffic as part of our broader system for identifying misuse of our models. Early deployment data suggests the classifier works well with real Claude conversations.

We will share our approach with the [Frontier Model Forum](https://www.frontiermodelforum.org/), the industry body for frontier AI companies, in hopes that this partnership can serve as a blueprint that any AI developer can use to implement similar safeguards in partnership with NNSA.

Along with the concrete importance of securing frontier AI models against nuclear misuse, this first-of-its-kind effort shows the power of public-private partnerships. These partnerships combine the complementary strengths of industry and government to address risks head-on, making AI models more reliable and trustworthy for all their users.

*Full details about our NNSA partnership and the safeguards development can be found on our [red.anthropic.com](https://red.anthropic.com/) blog, the home for research from Anthropic’s Frontier Red Team (and occasionally other teams at Anthropic) on what frontier AI models mean for national security. Click [here](http://red.anthropic.com/2025/nuclear-safeguards/) to read more.*

## Related content

### Anthropic acquires Bun as Claude Code reaches $1B milestone

[Read more](/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone)

### Claude for Nonprofits

Anthropic launches Claude for Nonprofits to help organizations maximize their impact, featuring free AI training and discounted rates.

[Read more](/news/claude-for-nonprofits)

### Introducing Claude Opus 4.5

The best model in the world for coding, agents, and computer use, with meaningful improvements to everyday tasks like slides and spreadsheets. Claude Opus 4.5 delivers frontier performance and dramatically improved token efficiency.

[Read more](/news/claude-opus-4-5)
