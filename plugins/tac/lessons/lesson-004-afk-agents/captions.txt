# Lesson 4: AFK Agents - Let Your Product Build Itself
# Video Captions/Transcript
# Duration: 46:27
# Source: https://agenticengineer.com/tactical-agentic-coding/course/afk-agents
# Segments: 93

Welcome to lesson four of Tactical Agentic Coding. In this lesson, you step out of the loop and you let your product build itself. How is this possible? We do this by adding four elements of AFK agents. This enables you to trigger your you to trigger your agents to run in their own environment. By the end of this lesson, you'll know exactly how to replace and outperform modern cloud-based agentic coding tools like Copilot, Devon, Jules, Codex, and other in-the-cloud coding agents. These tools are great. I've used them all, but they lack the detail needed to ship end-to-end in your code base with your engineering practices for your domain-specific problem. They all have natural limits because they're literally designed for everyone's literally designed for everyone's codebases, not yours. Just like the prompt, Your agentic pipeline is too valuable to outsource to a third party tool, especially this early in phase two of the generative AI age. You want to own your agentic pipelines. You want to be able to slice and dice your agents across your codebases, across the software developer lifecycle with ease. In this lesson, we put together the plan and build step of the software developer of the software developer lifecycle to augment and automate your codebases. Every tactic you'll learn was created to be simple, compressed mental frameworks you can use every single day for your engineering work. First, you stop coding. Our hands and mind are no longer the best tool for the job of writing code. Then you adopt your agent's perspective so you can maximize the leverage you get from your agents then you template your engineering so you can deliver consistent results in complex consistent results in complex codebases across hundreds of agent executions and now here in lesson four you stay out the loop what does it mean to stay out the loop there are two types of agentic coding both powerful both relevant but one lets you hand off exponentially more work to your agents. You've of course heard of human in the loop. When it comes to agentic coding, this is what most engineers are doing right now. The tech ecosystem has become obsessed with human in the loop systems and most loop systems and most fail to recognize the trend that there are increasingly more chores, bugs, and features that don't require your expertise at all. If you encode them, they're sitting on their device in the loop, prompting back and forth and back and forth, burning their precious time on work that could be handed off to the right team of agents in the right order with the right templates. This is classical in-loop agentic coding. And then there's of course Outloop. Outloop agentic course Outloop. Outloop agentic coding is off-device agentic coding. It's when you write high to low-level prompts that you pass off to your agentic pipeline, then you go AFK, away from keyboard. You walk away from your keyboard, or maybe you were never there in the first place. Maybe you sent the prompt right from your phone on GitHub, Slack, Notion, Jira, a text message, or any third-party tool. Stay out the loop, is a tactic of agentic coding that reinforces the fact that models will improve and models will improve and tools will change. This means over time, your agents will be able to solve more and more problems with every improvement. When you stay out the loop, you leverage this fact, enabling you to focus on scaling up what your agents can do instead of wasting time doing the work yourself. You build the system that builds the system. This is where we need to focus. Why is that? It's because this directly ties into our ability to improve our agentic coding KPIs. We agentic coding KPIs. We want presence down, size up, streak up, and attempts down. What's the best way to accomplish great agentic coding KPIs? You guessed it, stay out the loop and focus on building the system that builds the system. We're going to accomplish this by adding a net new agentic layer to your code bases. This is gonna be different It's gonna be a little challenging and it's going to be incredible if you take action on this. Here in TAC, Tactical Here in TAC, Tactical Agent of Coding, we're obviously capitalizing on this opportunity before it hits the masses. We're not playing today's game, we're playing tomorrow's. You might be thinking about how error-prone LLMs and agents can be or how AI can't possibly ship your special snowflake work end to end. Let me say this bluntly and clearly. If you are not wrong now, you will be. It's only a matter of time. Remember, tools will change and models will improve. That means engineers, improve. That means engineers, teams, leaders, and businesses betting on the future are the ones that will get ahead today. There's no way around this. Keep it simple and bet on the future. Lean into this. You want to be the one shipping more features with less friction, with fewer errors agentically, hands off, AFK. This means you get to product market fit faster and you expand your total addressable market faster with every day, with every improvement you make to your templates and to your templates and to your agents. Your current velocity is fractions of what it will be when your agents are doing all the heavy lifting for you because you've learned to stay out the loop and build the system that builds the system. Staying out the loop sounds great. how does it actually work and how can we differentiate from the plug and play outloop agentic coding tools that already exist with everything you've learned so far here in tech we can automate nearly half the software development life cycle and go from prompt to go from prompt to pr in lesson three you learn to template your engineering investing in your templates is mission critical for outloop agentic coding because when you template your engineering you encode your engineering solutions into your codebases for your agents. Notice how it keeps stressing the uniqueness and the ownership of your code base and your agents as much as the big gen AI companies building these Outloop apps and tools want you to believe this one size does not fit size does not fit all, especially as your product grows and becomes unique and differentiated. In this lesson, we chain your templates together inside the highest leverage point and the highest composition level of agentic coding, the ADW, AI. developer workflow. An ADW is a reusable agentic workflow that combines code, agentic prompts, and agents to deliver results autonomously. On the battleground of agentic engineering, the ADW is engineering, the ADW is how you create this new agentic layer around your codebase. It's the synthesis of previous generation deterministic code and new generation non-deterministic language models, prompt chains, and now agents. The ADW can be thought of as an agentic pipeline or very loosely speaking an agentic workflow. In the future these will just be known as scripts and will fully expect agentic behavior by default. We're creating a default. We're creating a concrete term for this pulling it from principle to decoding to be absolutely clear about how powerful and differentiated this unit of engineering is. AI developer workflows are what we'll use to automate not just the software developer lifecycle, but all engineering work and all workflows you want your agent to operate on your behalf. If you understand this lesson and start putting these ideas to work in your codebase, you'll fast track your transition into the future of into the future of Agentic coding, while other engineers are sitting at their device, prompting back and forth and back and forth, wasting time on problems their agents could solve. While they're doing this, you'll have built a new agentic layer around your code base where you stay out the loop and let your product build itself. In order to build Outloop systems, we need AFK agents, agents that run while we're away from the keyboard. There are four elements of AFK agents we need to build our Outloop system. This is Outloop system. This is what we're going to focus on here in lesson four. The four elements are prompt input, the trigger, you'll need an environment, and then a review system. These four elements can be remembered with Peter. These make up the four elements of AFK agents. Peter, prompt input, trigger, environment, review. With this setup, your agents can run while you're AFK. AFK agents are the agents you'll build to run the loop while you stay out of it. In this lesson, we'll run two ADWs we'll run two ADWs to automate the plan and build steps of the software development lifecycle. We'll go from prompt input to review. So this is our first ADW. We wanna keep it simple, but I also wanna show you the power of having your own dedicated environment. So here's what we're gonna use for these four elements. For a prompt input, we'll use GitHub issues here. We'll create new issues. and the title and description will be the prompt that our agent will execute on. For our trigger, we're going to use GitHub webhooks. I have this configured here. have this configured here. All the information you need to set up your own workflow using the elements I'm going to run here are going to be linked in your loot box. But I recommend you just watch to start and follow along at a high level. Don't get bogged down in the configuration of this. Just watch this through first. and then start setting up your four elements. That's the trigger. I have a GitHub web hook set up. And then for the environment, I'm going to use this Mac mini here that my agent has full control over. If we open up screen sharing, you can see I have an agent ready and waiting, agent ready and waiting, looking for requests from the web hook right here and we'll dive into exactly how this works in just a moment but this is our environment we're looking into this device right here and when we kick off our prompt input you're going to see this start up and boot up in just a moment here and then of course our review system is going to be github pull requests so after our agent finishes the work we've asked for with the prompt input it's going to turn around do the work in its environment and then submit a pull request back for us request back for us to review. Instead of diving right into the TAC4 codebase, let's just look at how this simple Outloop system works end to end. Let's of course start with our prompt source, GitHub issues. For the TAC4 codebase, I'm gonna go ahead and just create a lightweight issue for us to understand this process. I'll say ADW documentation. I'm gonna type slash chore to document the ADW directory, read everything. in ADW then update read me with information how me with information how it works. Okay. Chores are the simplest unit of engineering work that your agent should be taking off your plate with no debate at all. So let's kick off this ADW and let's watch what happens inside of the agent environment inside of this device here that we can see. So I'm going to kick this off. We're going to create a brand new issue here and check this out right away. Your agent is getting to work for us. So If we click here, we can see we're getting GitHub issue comments. We're getting a comments. We're getting a live feed of the progress of our agent. You can see it's classified the issue as a chore. In our previous lesson, we looked at chores, bugs, and features. This is how we can specialize certain agents and certain prompts to solve different problems in your codebase, right? So we have a brand new branch and it's starting to work on the implementation plan. You can see the software developer lifecycle planner has taken over and it's going to start shipping. This is all running on this device here. As you can see here, it's can see here, it's posting comments and it's working through the current implementation plan right now. So this device, this agent is doing this work autonomously for us. Not only that, we're getting a nice live feed of what's actually happening. By separating our agents, we're able to isolate the big three context model in prompt to solve one problem and solve one problem well. Just in these four steps, we have several micro agents doing their work, solving a specific problem well, right? So we had a prompt to classify this prompt to classify this work for spending up a new branch. And then of course we have an implementation plan. And so our planner, you can see our planner is completely done. It's committed its work right here. We can see that it's created a brand new spec file agentically. And now our implementer agent is going to start implementing the solution. So its context window is completely free. We can select a more powerful model, which I recommend you do for the implementation. And we can, of course, use the plan that was created by our planner as the our planner as the prompt for our implementer. So you can see how all these elements, every piece of our ADW, our AI developer workflow, it all depends on the fundamental principles of AI coding and of agentic coding. These ideas, these concepts, they don't go anywhere. They go everywhere, right? These are ideas that will always exist and we need to Pay attention to them with each prompt, with each agent. The details are everything. There's a reason why a lot of agent encoding tools don't work. It's because they don't understand your codebase. They don't understand how you work. And by encoding them into our encoding them into our templates and then chaining our templates, our meta prompts and our reusable prompts together, we get an ADW that can do work like this, okay? You can see the solution was implemented and now we're committing the implementation. Our agent working on its own device here is running through this ADW, this AI developer workflow. It's creating a PR. All this work is happening without us. Okay. But it's happening as if we were building it because we've templated our engineering. Okay. Success is engineering. Okay. Success is absolutely planned. You can plan success into your codebase by templating your engineering. Here comes the PR. There it is. Okay. So ADW. completed. If we scroll up, we can see how long that took. This simple task took a total of five minutes. And you can see our agents did all the work there. All we did was define the work that we want done. and gave it a little bit of detail, right? So this is a high level prompt that gets turned into a plan, right? So it's expanded. Our agent is filling in the details of our chore, just of our chore, just like in the tack three lesson, our chore is getting expanded into a full plan. And then our implementer is building it out. All right, so now we can check out what was done. All right, so if we click this PR, you can see top to bottom, a just a great clean write up of the work completed, right? So this PR address is 31. You can see the plan there. And then you can see the changes that were made. Okay, so if we scroll down, you can see everything looking good. And you know, there are issues here that I need to clean up that to clean up that I need to encode, right, we have a gentic developer workflow, that's just wrong. So this is something that I need to improve in my prompt to be clearer about what this is. But you can see here, if we go to files change, we can see just those files we asked for to be updated. There's the readme. We can quickly just scroll through this. I don't wanna get too hung up on the details here, but you can see we have changes for the readme detailing exactly how everything works. For the ADWs, there's the webhook that's running this workflow, cron, and this workflow, cron, and then the build step. and then the actual ADW, which we're gonna dive into in just a moment, environment variables, prerequisites, so on and so forth, right? So there are no limits here, right? Your ADWs can operate on your ADWs. So, you know, don't get too meta, let's keep it simple. You can see more details there. And then of course we have the plan, right? This plan was built by our planner agent and then handed off to our implementer agent. So this was the, literally the prompt for our implementer agent. So you implementer agent. So you can see top to bottom there exactly what that looks like. So this is awesome. This all happened with Peter, the four elements of AFK agents. For the prompt input, we use GitHub issues. For the trigger, we set up a GitHub web hook. You can see that right here. I'm gonna blur out my URL here. You can set up your own web hook right here. And then our device operating right here picked up on that and ran our ADW, our plan, build a developer workflow and then it submitted a PR back to a PR back to our review system and we reviewed the PR. So here we just have two steps of the software developer lifecycle fully automated end to end. I hope you can see how powerful this can be, right? just halfway through TAC, we can automate the key steps of the software developer lifecycle. Obviously, we're gonna take this a lot further. We're gonna push this, but right now, let's dive into the details of how this works so that we can see the ADW, AI developer workflows are in fact a composition of key a composition of key primitives that we can work with to build an agentic layer around any codebase you operate in. So keep in mind as we work through this that these four elements They can be anything, right? I'm not here to tell you what tool or what environments to use, right? What sandboxes to put your agents in. I'm here to give you a top-notch framework and tactics you need so that you can build a system that works for your codebases, your products, and your team. You will, and you should have opinions about how to structure your AFK to structure your AFK agents. Don't read into how this is configured too much. Focus on the composable units. All right, so let's dive into this. Let's run this again, only this time, Let me go ahead and kill the web hook so we don't get this kicked off. So I'm going to disable this agent here because this time we're going to update so that our environment is my local device and my trigger will be a local script will run right out of the TAC 4 codebase. So let's go ahead and open up the terminal. And this is a great place to follow along if you're interested. If we type ls, you can see we have can see we have the previous three TAC codebases. Let's go ahead and get clone in the lesson for repository. I'm going to cd into TAC 4. and then open up whatever your favorite ID is. I'm gonna use VS Code. Let's go ahead and boot up Claude. I'm gonna run in Yolo mode because I understand everything this codebase will and can do. If you're ever worried about the permissions, always check the settings file and make sure that things can't run that you don't want run. This is going to add additional control clear and there's no reason to run installation by hand anymore so i'm going to run slash install and run slash install and let's get this code base set up so whenever you run these commands and new code bases always always you know check out what is actually inside the file you can see here we've added a little bit more to this install command for this codebase we're reading reading and executing we're running several steps we need to unset the remote origin here, set up a new GitHub repo so that you can use your own GitHub issues and PRs. You, of course, can't make PRs to this codebase. It needs to be reusable. So we're doing that here and you can see our agent working through all this. And then we are copying our environment variables. We have a new top level environment variable file we need to set up. We're going to do that same installation of front end and back end dependencies as before. And then we're gonna copy our previous codebase.env file for our server. You can see the environment variable just got created there. Our .env is set up. So a cool thing you can add into your prompts is a report feature. It's gonna report feature. It's gonna set up some things, right? It's gonna run all these previous commands. And then our agent is going to report some results to us, to the engineer. to your coworkers executing this script, right? So it's gonna have some useful information, you know, work completed and then action required. Okay, so it's telling me, you know, I need to fill out the root level M and app level was already copied from tack two and the API keys are all good there. So we don't need to make any changes there. But then to set up our AFK agent, we have this detail here we need to set up a remote repo so that we can of course, you know, of course, you know, get access to issues and PRs to satisfy the prompt input and the review system elements of AFK agent. So I'm gonna go ahead and do this right now. If you're following along, you're gonna wanna do the same, create a new GitHub repo and you know, set up and run these commands. So I am operating on the type four codebase. So I'm going to reuse the same URL. So I'm gonna run this here. This of course won't work for you. Okay, so this looks great. Let's have our agent kick off a workflow for us. So how does this new agentic layer of our codebase work? It all happens here, as all happens here, as you could guess, in the ADW's directory. Let's finish configuring and getting everything set up, and then we'll run this. So in the dot environment variable file, the key thing you need here is these two. So you're going to need to set up your Anthropic API key. The agent SDK currently only runs with the API key. This may change in the future. Keep your eye out for that. I'll make sure there's links in your loop box mentioning if that's changed at all. But so I'm going to set this and for your cloud code path, depending on your configuration, this might not work right away. not work right away. So you need to run something like this, which cloud copy this and just paste this in your environment variable right here. I'm going to set these up. I'm going to cut this part of the video, obviously. So the environment variables are set up for the top level. That's great. Now, before we dive into this, let's go ahead and create a prompt input for this workflow to run on this device. So we'll go ahead, open up GitHub issues. We'll hit new issue here and let's have our agent build out a new feature for us. So I'm going to type just on L support. And let me quickly refresh let me quickly refresh our memory here on this codebase. We can execute it with sh script start. All right, so this is going to kick off our backend and our front end. There it is. So let's go ahead and open this up. And as you can see, we have our classic natural language interface. We can upload test data, we can get users. We can then query these users, users age descending. And then that query will come through here as an SQL statement against our local SQLite database. It all looks good, right? So what we wanna do here this new feature we're going new feature we're going to add json l support if we upload you can see right now we can only do dot csv or dot json so let's kick off this adw we're going to dive into here in just a moment to operate on this for us and so let's go ahead and kick this off the first thing we need to do of course is write the prompt input right this is step one of the four elements of outloop systems of afk agents so i'm just going to write up a prompt here this is what our prompt is gonna end up looking like here. We're just adding details. This is a high to mid-level prompt where to mid-level prompt where we detail what we want done. Adding support for uploading JSON-L files. And then we have a couple additional details here. Standard, lib only, no new libraries, concat nested fields, handle arrays, update the UI. Okay. Brand new feature coming in, right? Net new. And we have disabled our agent running in its own sandbox here. The webhook event is off. It's not listening. So this won't run. We're going to kick this off on our device with a local trigger. All right. So trigger. All right. So we're going to create this issue here. And now we can reference this issue by pooling number 33. Let's first. validate our environment by running this health check command. This is something I highly recommend you build into your ADWs before you run, you know, your complex or simplistic workflow. You just want to know that everything's configured properly. All right. So we can do this by opening up our health check and we can run this command. So I'm going to copy this paste and Then I'm gonna kick off this health check. These are health check. These are all single file scripts, by the way. The ADW directory is really important because it operates as a unit around your codebase. Notice how app is completely untouched here. We're not combining or conflating the functionality between these two. the ADWs combined with your prompts, as you'll see in a moment here, you can see all of these reusable prompts that feel the ADWs, these two directories and their content form the agentic layer around your codebase. So let's run the health check. We'll dive into this. It'll all make sense in a moment sense in a moment here. So let's run this health check. This is going to be on issue. What was that? 33. Okay. So we're going to run on issue 33 and let's see if we have everything set up properly. So environment path is set up. GitHub repository looks good. We do get a warning. This is a warning for you, the viewer. If you're still pointing toward the original attack for less encodebase, you're going to need to update this, right? So that you have access to your own pull requests and you know, issues and whatnot. So that's good. We can see Claude Code runs properly. We posted runs properly. We posted a health check onto the comment. There it is. And we are good to go. We're good to run our AI developer workflow. So let's kick this off and then let's break it down. So if we open up the top of our ADW plan build, we only have one, ADW in this codebase, just the plan and the build. Remember we're automating the plan and build step of the software developer lifecycle. And then we can see the exact same thing, right? So super simple to kick off. We have UV run this, okay? These scripts, right? These single file scripts can be anything you want be anything you want using astral UV to run Python single file scripts. You can see you have a couple of dependencies there. These can be bund scripts if you want, shell scripts, whatever you want. The whole point is that they're a layer on top of your codebase. So let's go ahead, copy this command, paste. Let's get rid of the ADW ID. We don't need that. And let's just run 33. We need to make sure we're going into the ADW's directory here, ADW slash, kick it off. And here we go. So just like in our environment, it's running that exact same workflow now, exact same workflow now, right? The only difference is I kick this off with a manual trigger. It has its own unique ADW ID to identify this AI developer workflow. And now it's got its own branch, JSON-L support. Now it's gonna do this work for us agentically. You can see the branch that we're operating on in this code base just got updated. Our agent has taken the wheel. We are now out of the loop, right? We said what we want done and now it's starting to do the work for us. We're focusing on the what, not the how. The details of how things are done, we've already encoded done, we've already encoded into our codebase via our reasonable prompts, our templates and our meta prompts, right? That's all here. We'll walk through these in just a moment here. So if we hop back to the issue, you can see our agent is updating this live, just like our previous. The issue has been classified as a feature. So instead of running the chore, it's going to run the feature here. This feature template meta prompt is going to build out a entirely different set of instructions and a different template for our agent to fill out and operate on. We can target specific classes of target specific classes of problems with this switch, right? With this feature inside of our ADW that classifies is this a chore bug feature or any other specific class of problems that you solve in your codebase. All right. This is a very important pattern of agent to code and you need to be able to target specific classes of problems. So while our agents building this out on our device, let's go ahead and break down how exactly this works inside of the codebase, right? How is your agent doing this work for us agentically? It all starts in the ADW starts in the ADW plan build. Here's the workflow, right? It's right here at the top of the file. Very easy to understand. Fetch the GitHub issue details, right? So pull in the prompt source, creating a feature branch for this work. This could be a chore or a bug as well. We're then running our plan agent. There's gonna be a clog code with a specific plan prompt. It's gonna run our slash feature reusable prompt. Then we're gonna have our build agent take the plan and based on the plan, it'll pull in all the right context. Again, the right context. Again, we did this in our previous lesson with the slash implement command. You can see that right here. We're going to pass the plan in as an argument and then it executes. All right. And then finally, of course we create a PR. So right here, we're automating the first two steps of the software developer life cycle plan and build plan and code with this ADW right? A developer workflow plan, build. Let's break this down. You can see here at the top, we have different agents for different specific tasks. Let's go ahead, collapse everything and just run everything and just run to the main method. What's important right now is that you understand that you can put together these composable units of classic deterministic code with these new units of intelligence with our Claude Code agentic coding agent. It's all about that. And then reporting, making sure that our process is observable and understandable from a higher level. It doesn't matter if our agent can solve every problem if we don't know that it's solved. We need to observe, we need to report, we need to log and monitor. What we have here What we have here in this ADW is that exactly. We're putting together these elements, these composable units of agentic engineering, and we're letting the product build itself with this new agentic layer around the code base. So let's just walk through this. It's relatively simple. Parsi arguments, hashed in, the issue number. You can see issue number 33 right here. And you can see the steps that our agent is currently on. If we don't have an ADW ID passed in, this is optional. We make one. We're then setting up our logger, checking the environment variables, and then environment variables, and then we're getting our GitHub repo. We have different modules, of course, that we're pulling in data types, GitHub and agent. Then we fetch the issue from GitHub, right? So this is gonna pull in. title, our description so that we have the actual issue and we can see this type very plainly here, right? We've just got this all typed out. The key thing here is the title and body. All right. And then we have this pattern of either reporting the issue. You can see all the issue comments here and by issue, I mean the issue comment, right? So we're adding a comment on the GitHub comment on the GitHub issue. or we are checking the error, okay? And so in between all of our logging and reporting, we're just running the individual steps of our ADW, okay? So here's that classify issue. We can go ahead and look at that, right? Classify issue, check this out. This is our prompt that we're running as a slash command. And we're just going step-by-step. We're isolating all of our prompts so that we can improve them. No ad hoc prompts as strings inside of the codebase. Everything's isolated so that we can improve it. We have improve it. We have a nice mapping here. Of course, if you have more detailed or different classes of problems, you just add them here in your classify issue step. And then you update your data types to support the different types of slash commands that you solve. All right. And again, this is just one way to build out an ADW. We're just walking through this so you can understand how you can build these out for your code base, for your problem, for your application, for your team, for your users. Right. So there's that. We then We create a Git branch and again, it's the same process, right? Git same process, right? Git branch is created by our branch PR. Notice how with every one of these templates, we're encoding our engineering into reusable prompts that our agent can tap into, okay? And this is all about that same idea, that same tactic. We need to adopt our agent's perspective behind that We need to give our agent our perspective, right? They need to see what we would see when we're solving that specific step of the AI developer workflow, right? Because previously, these are just developer workflows. This is what you and I are you and I are doing on the ground every day. But now we can add this new agentic layer and then we can augment and automate a lot of these processes, right? That's our branch functionality. And then we have our build plan. So let's go into build plan. And let me show you what one of these methods actually looks like. We're building a template request, right? So we have our agent name, the slash command, right? And remember the slash command activates the right prompt and then the arguments that go into the prompt, right? Specifying the model when you run this and when you set up your ADWs, I up your ADWs, I highly recommend for your build and implement steps, right? You build and implement, you use the most powerful model you can just for presentation sake. I'm using Sonnet just cause it's going to be a little faster, but you're going to want to throw top models into the most complex steps, which are going to be your plan and your build step. Build plan, you know, we're just going to kick off this slash command, run some debugging logs, and then we're going to execute it, okay? And all the way at the bottom here, the execute step is, of course, in our agent module, which is going to build the prompt, to build the prompt, log the prompt, and then run the code using Claude Code in programmable mode. All right. This is why this is so important, right? We're kicking off cloud code right here like this. Be sure to read through this. It is running dangerously, but because it's in its own environment, this is going to be safe, right? And the key idea here is this is, this is the value of programmable mode. This is why it's so powerful. It lets us use Claude Code as a new agentic coding primitive, where we can call our agent with any prompt and any step of our workflow that we need workflow that we need to automate engineering work. All right. So this is all here. This is going to set up a great base framework for you to understand ADWs and build them into your own codebases. All right. So that's what this is doing. There's going to call the build prompt. There's going to call the write slash command with the planner agent. And after that, right, we just continue the flow. We then tap into whatever next step we want, right? So if we scroll down from here, we're checking errors. We're continuously updating on what the issue is on what's coming next. And we're making it observable. A making it observable. A couple really key pieces here that we're going to look at in upcoming lessons. We have our system reporting individual log files on a Cloud Code session ID basis. So we can come in to this system, this device, and really understand what's going on here at a very, very detailed level. We activated this thanks to Cloud Code hooks, but there are many mechanisms to tap into this functionality. So that's one mechanism in which we can observe and review. We also have agents that run on a AI developer workflow basis. This workflow has basis. This workflow has completed, right? And you can see all the agents, all the little micro to macro agents inside this ADW that have completed branch generator, issue classifier, PR creator, implementer, planner, right? It's all here. And then we have the committer for the planner and the implementer, right? And so you can see that if we continue to scroll down, it's all the same process, right? Run either non-deterministic intelligence or a deterministic step. We then check for issues, do some logging, do some reporting, and then we continue. and then we continue. Now we're committing the plan. Here's a git commit. And then we're implementing the solution. There it is. Plan file on and on and on, right? There's the git commit for the implementation. Then we're building out the pull request right here. This is our PR creator. If we tap this, we can see all the logs that our PR creator went through, right? So we have JSONL here and we have JSON for more easy readable format. So this is all here. This is how this ADW works. We'll dive into the details and we'll play with these composable units as you progress units as you progress through each lesson. But this is important. ADWs let us create a new agentic layer around our codebase that tell our agents how to operate and perform real engineering work. Okay. If we look here, this workflow is complete. There's the PR. We can get out of this. Look at this issue here. You can see this entire workflow took about 17 or so minutes. And there it is, right? So we now have JSON-L support. We can hop in here, review this. There's all the files changed. It's all getting reported based on our reported based on our PR review prompt. All right, this looks great. It handles all these key features. Comprehensive test coverage, love to see that. And then we can, of course, you know, review all this code, see the files changed, see everything that's changed. There's the front end UI updated. Here is some sample data, just like we asked for in our tests, complex data, sample data, and on and on and on, all right? And of course, very importantly, here's the plan. right? The planner created the plan and outputted a concrete asset that we can use to understand what our implementer will do. This is will do. This is really, really important. This lets you improve, right? It's not enough to just run a random ad hoc prompt because how will you know how to improve that? And more importantly, how will your agents know? how to improve that. You know, we can look through this entire plan. You can see it's very comprehensive. Our agent created a 170 line plan and then our implementer went and built this out. Okay. So we have an opinionated plan build workflow that fits our code base for building features. Okay. So, you know, we can hop into this codebase here. and see how this actually turned out because actually turned out because our agent was updating in this environment. You can see we're still on that feature. So what I'll do here is just boot up the client and server again with all this updated code here. And then let's just see what it looks like, right? Let's open up Chrome. Let's go to our natural language interface and refresh. You can see it's added a couple different test items. It was operating right on this SQL database. So that looks good. There are things there. Let's do a quick regression test, select users, descending, sign update, Let's fire that off. We should get our same prompt. There we go. prompt. There we go. Nothing different there. Order by descending looks great. And now let's go ahead and try to upload some. Okay, so let's check this out. Our UI got updated CSV, JSON or JSON L. That looks great. We have a new option. We can select event analytics. That looks good. Let's go ahead and drag and drop this in. On that PR, it told us where exactly that was. Let's drag and drop in that sample data. This is gonna be in test assets. So we can quickly just find that app server tests assets. Let's open and finder assets. Here and finder assets. Here we go. Browser drag and drop simple JSON data here. Scroll down. Let's see where our simple data is. Okay. So that's here. I think that was there before. So I'm just going to go ahead and just delete this and I'll delete complex data as well. And then I'll redirect and drop this in, right? We wanna make sure this is a fresh upload. Let me go ahead and hide this so we can see everything below. Upload, sample data, JSONL, drag and drop. There it is, sample data from a JSONL file. And you can see we have that exact same naming that exact same naming convention as specified. This was a feature that an engineer may have taken up, may have picked up themselves, and it was completely unnecessary, okay? I can guarantee you right now, you or an engineer on your team, you're working on things, you can fully automate by creating an agentic layer around your codebase that is driven by the four elements of AFK agents. Right. And doing all this will help you stay out the loop. There is no reason to do engineering work like this. And this trend will continue. OK, this is not specific to this code base. Yes. Right now it is small, but it doesn't matter. The size only matters so much. OK. And every time you miss something, what do you do? You don't fix the issue. you fix the system that caused the issue, right? You fix your templates, you fix your ADW. Okay, so anyway, let's finish testing this. Sample data, all rows, age between 10 and 50. Okay, so just a random query to just showcase that all of our data is here. This feature was created in one was created in one shot. We now have JSON L support. So that's there. That was created on this branch, on this machine. We set up the four elements and this happened end to end. And I just wanna stress this one more time. It doesn't matter if you use GitHub issues, if you like JIRA tasks, Notion boards, whatever you wanna use. The key is that these four elements are always there and you want to build systems that build the system for you. That's the key here. That's the key piece of information from this lesson before we push this to the next level. And what next level. And what is the next level? The next level is make this run reliably, right? Improve this, make our agents better, make them more consistent. How do we do that? We do this of course by closing the loop. More on that in a second. A couple of things I want to touch on here. We set up a GitHub web hook here, and then I exposed this functionality via a A proxy server, I'm using Cloud Flared. You can use anything you want, set up your own dev infrastructure. The key is just to set up a dedicated environment. I highly, dedicated environment. I highly, highly recommend you do this. You want a dedicated environment. that your agent can run in on its own. This way you can create a GitHub issue anywhere you want, or you can fire off a prompt input that hits a trigger that then kicks off your agentic workflows no matter where they're running. All right, this is super key, super important. We have two units of engineering work that happened agentically for our codebase, for the TAC4 code base. That's the key, ADWs and the prompt templates you build out for your codebase, for your codebase, they solve classes of problems that you no longer have to solve. You don't immediately start with top tier agent code and KPIs. You're gonna run some of these, you're gonna run some ADWs, you're gonna run some prompts, some templates that don't fully solve the problem. Then you'll spend some time, you'll invest in your templates, and then you'll compose them into your ADWs and you'll improve. This is the new layer, the compositional unit that I highly, highly recommend that you focus on now. Don't sit and prompt back and forth build up these workflows, your agent can workflows, your agent can solve many, many problems that you and others are still wasting your time building. Okay. I want to be really blunt here. I want to help you get ahead. And that means addressing the faults of where we currently are. All right. In loop agentic coding is great for experimentation. It's great for exploring new territory and it's great for very, very hard, very specific problems. Okay. But let's be real. Most of engineering is not that. So, If you invest a little bit of time here, setting up your reusable prompts, up your reusable prompts, right? Your templates, your meta prompts, and then creating some ADWs that once you set these up, they're just there, right? You know, for this codebase, this just solves entire classes of engineering problems now moving forward, right? You just solve that. these types of problems, they're just solved. All I have to do is write the prompt, the high level prompt, not even the plan, okay? So I hope you can see things coming together here. There's a lot to dissect, but the idea here is simple. Build a new agentic layer in your agentic layer in your codebase so your agents can run and ship engineering work based on your engineering problems, based on your code base, all right? By doing this, by dedicating and focusing really hard on these, you know, two essential directories, .clawed commands, aka wherever your prompts are stored and ADWs also known as compositional workflows are stored, right? Your agentic workflows, your agentic pipelines, your ADWs, AI developer workflows. So many names for the same thing, right? You wanna focus on You wanna focus on these elements and you wanna create this outer layer to automate the different classes of software engineering problems that you have. It makes you add logging, add observability. You wanna make it easy to understand where and how things are happening, right? We can hop into the issue and we can see this agent completed its work. Then we can review to make sure that it is actually done. You want to set up the four elements of AFK agents, prompt input, trigger, environment, review system. And this enables you when you put these things together, you can stay out the loop. For example, the loop. For example, this is one workflow, one Outloop system I have set up. This device now is always running for me. If I hop into this box, and I kick this trigger off again, this codebase is always listening to whatever web hook I have set up for any codebase now, and it will just start solving the problem. After the upfront, you know, install, right? And you can encode all the installation into a slash install command, right? After you get the setup, you have an agent operating its own environment. You can call using an out loop system. an out loop system. This is massive impact. I recommend you take action on this now. Like don't wait, the ROI here is insane. I want you to get to the aha light bulb moment you'll get when you run one of these end to end, all right? Start with something simple, you know, get everything configured and fire off a chore. What you wanna do is understand how controllable this is. Yes, it's true. There are cloud-based tools that do this workflow, but that's not exactly true. These tools are not running your templates, right? They're not running your They're not running your prompts. They're not built to solve your problems, right? Your classes of problems that every engineer faces. So invest in this. This is a critical, critical lesson, right? Start with great defaults. You know, for your prompt input, I highly recommend you just go with GitHub issues for your review system at the end, GitHub PRs, and then your trigger and your environment. Just start setting this up locally and then offload it to another device, right? You want to get to this point where you start with low hanging fruit, you know, build up trust and your ADW and and your ADW and your system that you're building. Start templating out the chore for your codebase, then move up to bugs. And then eventually, you know, you'll get to the feature level. Right. But you can definitely start small with just the chore. You know, these are simple tasks. You know you can hand off, then move to bugs, and then full-on features. Use this code base as a great starting point for understanding how you can build ADWs in your code base, right? There's engineering work that you just truly don't need to do anymore. So template your engineering and stay out the loop. Focus out the loop. Focus on this new agentic layer. this new outer layer on your code base so that your product ships itself. We now have a tactic that guides us toward building autonomous systems. When you stay out the loop, you prioritize agentic coding. We solve problem classes and we build a system that builds a system, right? Our systems, this agentic layer is solving the detailed problems while we're teaching it how to. In a lot of ways, we're automating the central units of the software developer lifecycle so that we can focus on what we want, the planning and the reviewing. planning and the reviewing. Okay. By templating your engineering, we increase our planning velocity by encoding solutions to classes of problems into the outer layer of our code base, the agentic layer. This is great, but there is a massive issue. These workflows, although powerful, they require lots and lots of reviewing at some level. We'll always need to review, but wouldn't it be great if we could increase our review velocity as we scale our planning and building velocity with our agents. In the next lesson, we're going to do exactly that. We're going to that. We're going to close the loops. We're going to give our agents multiple leverage points of agentic coding needed to validate their own work. We're going to teach them how to do this on multiple dimensions because what's better than one test? Yes, two, three or 10. All right. So in lesson five, we teach our agents how to test and validate not just back end work, not just scripts, but front end and all types of engineering. By doing this, increase our review velocity by teaching our agents how to validate their work great job here you're making job here you're making a ton of progress you are halfway through Tactical Agentic Coding take some time to digest this information there's a lot here this lesson is pivotal your ability to internalize lesson three and four will decide if you come out on top here template and stay out the loop i'll see you in lesson five where we increase the reliability of your agents by drastically cutting down the time you and your team need to spend reviewing and fixing agent outputs. I'll see you in lesson five.