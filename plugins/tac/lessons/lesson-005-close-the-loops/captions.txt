# Lesson 5: Close The Loops - More Compute, More Confidence
# Video Captions/Transcript
# Duration: 53:53
# Source: https://agenticengineer.com/tactical-agentic-coding/course/close-the-loops
# Segments: 108

Welcome to Tactical Agentic Coding, Lesson 5. Let's be brutally honest with ourselves. As engineers, we often make the mistake of thinking the most valuable asset we create is code, architecture, systems, plans, or features. This is wrong. Our most valuable contribution is the experience we create for experience we create for our users. That means one of the most valuable things we can do is making sure that all of the engineering stuff, the code architecture features, do what they're meant to do in the first place. How do we do that? You guessed it. We test, we validate, we close the loop on the work done so we know for a fact that the experience we designed for our users is what's in our users' hands. Agentic coding presents us with a massive opportunity. with a massive opportunity. The opportunity to have your agents test on your behalf like you never could at scales you never will achieve. This is the gift of generative AI. This is the gift of the agent architecture. There is one question that guides this lesson that unlocks this opportunity for you. This question is key to unlocking one of the most powerful leverage points of Agentic coding. This leverage point gives your agents the ability to save you hundreds of hours by automating tests and review automating tests and review workflows that would otherwise cost your time and attention. All you have to do is answer this one question. Given a unit of valuable work that's production ready, how would you, the engineer, test and validate this work? If you can answer this question for every class of work that your code base handles and then encode the answers into a command or tool call, you will fly while other engineers run. It's that simple. And depending on your staging and production system, it's and production system, it's that complex. This is where code-based architecture comes into play as a massive critical leverage point. Net new codebases have a massive advantage here. So let's go ahead and answer the question. When we ship code to production, how do we validate that our work has shipped successfully? If you think through this, you'll find a concrete flow of steps and tools that you go through to know that your code base has shipped successfully in production without errors. For example, you might run your linter, you run your linter, you might execute unit tests, you'll run UI tests, maybe your CI CD, runs your integration tests, you likely build or compile your application. Maybe you look for specific log messages in Datadog to confirm your features live, or maybe you check Sentry to look for a lack of error messages. If you're training models or doing data science work, maybe you run a custom evaluation that ranks your artifacts performance. And maybe you have an LLM as a judge workflow. And here's the big one for many engineers. You probably opened the You probably opened the browser and clicked through your new feature. What a waste of time, okay? These are all feedback loops. Let me explain why this is a waste of time, okay? These are things that you and I will do less and less and less as we scale our agentic systems. These are all feedback loops you can now hand off to your agents. This is all work you don't have to do anymore, but likely still are. Why are we still doing this work? It's because we're missing testing as a critical testing as a critical leverage point of Agentic coding in your code base. That brings us to the lesson five tactic. The tactic here is dead simple. It builds on a principle of AI coding you've likely heard before. The lesson five tactic is always add feedback loops. Your work, my work, any engineer's work is useless unless it's tested. The ultimate test will always be your users. The next best test is you. Well, it used to be. Now, the next best test is an army of agents validating your entire agents validating your entire codebase with regression tests and most importantly with end-to-end tests. I'm not saying stop testing or stop reviewing. I'm saying start handing off this responsibility and start teaching your agents to test. Why should we do that? Why should we hand off more to our agents. Of course, when you do this, you create closed loop feedback systems where your agent can execute, validate, and reflect on the work done in a loop until the job is done. By teaching your agents to teaching your agents to test, you continue to dial up the autonomy level of your agents in your codebase. We continue the trend of building out the net new Agentic layer around your codebase. This is us building the system that builds the system. Keep this idea in mind because this is the differentiating factor between Agentic Engineers and engineers of the past. Let's be clear in our terminology here. When I refer to in-loop Agentic coding, I mean you sitting right here, prompting with your agent back and forth. agent back and forth. Outloop is of course a high level prompt running through the Peter system that fires off on your isolated device, on your isolated machine. We're going to fire off an AFK agent in this lesson that's gonna run our new full pipeline. Now, closing the loop is letting your agent operate on work, calling a command or a tool to get feedback on the success of the work. You then take the feedback and your agent continues to build until the feedback is positive. This is what it means to close the loop. When you do this, you let the this, you let the code write itself. You let the agent operate with the right information so well that it closes the loop. Now with testing, one of the most heated debates in the engineering world has come to an end. Now engineers that test with their agents win. Full stop, zero exceptions. This is because the value of tests are multiplied by the number of agent executions that occur in your codebase. Testing is one of the highest through agent leverage points of Agentic coding, you can use. Why is can use. Why is that? It's because it lets your agent close the loop and self-validate. It lets your agent know that the work was done right. This leverage point is so important. It has an entire lesson to itself. All right. And this makes sense, right? If your auth tests are passing, you can be more confident your authentication system is working. If your UI tests for your chat support system are passing, you can be more confident that your chat support system is working. If your end-to-end tests on your staging environment are passing, you can be very you can be very confident every system is working. And with every passing set of tests, you free your context window and you stop second guessing so you can focus on what's next for your users. It's no different for you. or for your agent. If your agent sees something wrong with the auth test, you can't ship to production. It has to dive into the system and fix it. Test, let your success scale. Successful codebases will grow. As you're testing every net new feature with your agents, you can scale with confidence. With scale with confidence. With Agentic coding, test, let your agents scale your success. All right, same game, different day, new tools, exponential impact. Let's leverage them. Testing at its core is simple. Did it do what you designed it to do and did it break anything else along the way. In Tactical Agentic Coding, lesson five here, you'll learn to hand off the test step of the software developer lifecycle to your agents so you can continue your journey for great Agentic coding KPIs. Remember, we want attempts Remember, we want attempts down, we want size up, we want streak up, and we want our presence down. How do we accomplish this? With upfront investment into the new Agentic layer of our codebase. Remember, we want end-to-end agents, not good agents, not great agents. I mean, we don't want them in the loop, although there's many times where we still need to be in the loop here as we're building these systems out, but we don't want to stay here. We don't want to be babysitting our agents. We want work done agentically. Let me show you why we always add feedback loops all right feedback loops all right so let's go ahead open the terminal if i allow us you can see our previous four lessons we're going to clone in the lesson 5 codebase and get started this will be available to you link in your loop box below i'll cd into tac5 and boot up code in this directory all right let's open claude in yolo mode And let's get the application agentically installed. So I'm gonna run clear slash install. So of course you can trust me, but always review these slash commands. As mentioned, these reusable prompts are gonna become more and more become more and more like scripts, but you want to be reviewing exactly what's in them because they have massive control. If we open up the slash install command, you can see exactly where this is and exactly what this looks like. Reading sample files as before, priming our agent. And then we're running a bunch of commands that we've run before in the past with the exception of a couple of things here. We're now going to reset our database so that our SQL like database for application is automatically built up for us. We updated the copy.inv command to also copy the to also copy the environment variable from the tack four lesson. So this copies both the app environment variable file and the tack four environment variable file. So you can see that just got copied in there. That all looks great. Our agent is working on all this code for us. It's setting things up. There's that database set up. The backup got copied to the database and there we go. it's starting the server for us. So you know, you can just keep piling on these powerful, reasonable prompts for yourself and for your team. And so there we go. It's now validating that that work was done. Looks great. And done. Looks great. And then it's going to report a bunch of stuff to us here. While this is running, let's discuss a couple things. So there are many ways to test. In this lesson, we're going to cover common feedback loops you can give your agents use these as guides to understand what you can do but while you're looking at this while we're working through this don't limit yourself remember everything is one tool call away tools are just functions and functions can do anything we'll work through these capabilities using in-loop Agentic coding like we are here right we are actively prompting back and forth prompting back and forth to our agents this is for presentational purposes as you progress You wanna be doing less and less of this. You wanna stay out the loop, kicking off workflows that run in an agent environment. And so speaking of that, let's go ahead and fire off a complete workflow where our agent is gonna run the plan, build and test step of the software development lifecycle. It's gonna take some time to run. It's gonna build a full feature and test it for us. So let me just go ahead and kick this off. So you can see our agents primed here. That's great. If we look at If we look at our ADWs here, We have a new set of ADWs we're gonna break down later. What we wanna do right now is just kick off the full flow that we have built up up to this point. You can see we've improved the application structure. Right now, all we wanna do is spin up a new Outloop AFK agent using the PIDA framework we discussed in the previous video. So how am I gonna kick that off? I'm gonna open up GitHub. And I'm just going to kick off a brand new issue here. Alright, so I'm gonna hit new issue. This is my trigger. And so I'm trigger. And so I'm going to write a prompt and put here and let's go ahead and do this random natural language query. Let's also open up the application to get a refresh. And what we want to do here is just add a simple button. We want our agent to handle all this work for us. We don't need to be doing any of this feature building work. But we're going to build a new feature where we click a button and we want to have a brand new natural language SQL query created for us that helps us just get started with querying in our test application here. All right, so I'm gonna type this ADW plan build this ADW plan build test. So this specifies the ADW we're running. I'm gonna say feature. I'll write the prompt here. So create a new button. All right, so I have a kind of high, maybe mid-level prompt here detailing some work. And this agent, this Outloop AFK agent system is going to handle this for us agentically. So this is gonna run on our agent device here. Our agent has full control over this machine. If I open up screen, you can see exactly what's going on here. So I have my agent's machine right here, dedicated box just for my agents. I for my agents. I can close this. I can open up code and you can see here we are listening to our GitHub web hooks. This is the trigger that's going to kick off this agentic workflow once the prompt input comes in. Let's go ahead and kick this off. And then you're going to see this kickoff as that web hook event comes in. There it is. There's a lot of work that's going to happen here in the background. You can see our agents getting to work there. We've made some improvements. We'll get around to these. This is going to be a long running Outlook job. We're going to have a slew of chained together agents operating. chained together agents operating. They're going to build plan and test. So we'll go ahead, minimize this, let it do its job. You can see our agents messages starting to trickle in. We made some enhancements. Again, we'll cover this as it's working out. It's building the implementation plan. Now with the planner, let's let it do its thing. So while our plan build test agent system is running in the AI developer workflow plan build test, let's go ahead and work up to the work that's happening in that Outlook system. All right, here we are. We have just primed an agent and primed an agent and it ran our install commands. Okay. And so it set up the front end codebase. It looks like I had a previous port in use. Let me just go ahead and kill that and reset so I can run sh scripts, stop scripts, start. I wanna run on that port so there's no confusion down the line. There we go. There's a front end, there's a back end. Okay, so I do need to reset the remote URL. I'm just gonna go ahead and run a prompt for this. I'll just pass this in here. I'm going to set this back to the TAG5 codebase. If you're following along, go you're following along, go ahead, create your own separate repository and then set the origin to that. So here we go. This is going to automatically set here. Fantastic. So let's write our first closed loop prompt. So this is a prompt that's going to validate itself. Hopefully you've already seen these and you've used them, but I want to start here to set up a great foundation to how we'll scale up to multiple closed loops. I'm just going to open up a blank file. I like to do this when I'm creating prompts and I'll just type this. So update. So we'll open up the SQL processor here and I just want to do just want to do something kind of random to showcase how the closed loop system can catch and resolve errors quickly. I'll copy the reference. I'll use the at symbol for Claude code to automatically know and read in this file. I'll say comment out validation and then I'll say validate your work with. So notice the structure of this simple closed loop prompt. We have our request, we have our validation and then we have our resolution. All right, so I'll just copy this. And then I'll fire this off inside fire this off inside of our agent here. Super simple AI coding change, right? We're commenting. And then we're going to run rough. As you can see, rough picked up an error. We now have a unused import. Rough is looking at that change. It's going to provide input back to our agent via standard out. Our agent picks up on that as it's context window and then reruns rough check. It's done. And now it's completed the work that we wanted. Okay. So no issues. This is the simplest closed loop that you can imagine. All right. Now the structure of this is structure of this is very important. This is unlike a standard prompt. This prompt instructs the agent to validate itself. It creates a small loop system. Okay. So let's scale this up and let's add another feedback loop. Right. So I'm going to run slash clear here. Let's replace this prompt here with an updated prompt. We're going to update the server file, remove, the insights endpoint. So if we go here, if we collapse everything here, we have this unused import here. Let me go ahead and collapse even further. We have this insights right here. And so this is not being used. We want to get rid of this. And then we're going to then we're going to move our load and call to load on environment variables right before it's import. So there's the request. And then here's our validation validate multiple things. So we're opening up a couple loops here that our agent now has to close. Okay. It needs to run these commands, look for errors. If there are resolve them. All right. So rough py test and then a classic Python compile. Okay. Again, we're just going to copy this. We have a new clear dot instance here. We're going to run this and let's see what our agent does. So we have a linter, we have a unit tester and we unit tester and we have a compilation command. All right, so we're starting to stack up feedback mechanisms for our agent, okay? This is still just the beginning of what we can do with this. Here we go, let's watch what our agent does here. So we have this insights endpoint here, and this is gonna get removed momentarily. So it moved the load, and now it's gonna run rough. There it is. So there's an issue here, right? We can't have load env above an import. Ruff found this. So it was passed right into the standard in of our agent and our agent is going to move this. It's gonna move this. It's gonna resolve the issue. There it is. Our agent added some duplicate imports. This happens. Okay. Now it's gonna run Ruff again. So check this out. It ran it again, right? It's validating its test again. And now it's continuing to the next step. Now it's closing the next loop. Okay. So now we know that our code is formatted. Here we go, PyTest. And so PyTest is running. Looks like there is an error here. Let's see what the error was and what our agent does. Okay, so it sees that that line is commented out and it picked up on the bug that we created in our last prompt, right? So it sees that the validate SQL query has validate SQL query has been commented out. I need to uncomment this and call a function. So there's that import coming back in and there's that comment getting resolved. So without knowing what we previously did, right, thanks to the context window clearing, it fixed this issue and made sure that all the pie tests ran. And this is important. You have to make a rule inside of your codebased architecture. What's more important, the code or your tests? The answer should be your tests. Your tests should be the rule of law in your codebase. If your tests aren't right, focus on having your agents update the test so update the test so that your testing commands have weight, right? If the tests aren't right, things must be changed. And there's our Python compile. These all work. We have three feedback loops that our agent is tapping into to make sure that the codebase is running properly. Okay. So we saw there it fixed two issues, you know, and you're gonna see this, right? As the agents progress, as the models improve, we will be the ones prompting the errors. We will be the ones asking for things that aren't quite correct. And these quite correct. And these tests, these closed loop validators, they're going to help us by helping our agent keep all the work that's done on course in check with the standards inside of your codebase. Okay. Now this is just a small, simple micro example, right? We can continue scale this up you'll notice here you know two things two classes of testing are missing from our tests and there are many many tests you can add here it's all about providing feedback for your agent this is going to be different for every engineer depending on the type depending on the type of codebase that you're operating on your language the problem set you're tackling and the way your codebase is architected but the ideas are the same you can provide feedback your agents inside of your agentic prompts by giving it feedback and by following this three-step framework for writing closed loop prompts there's a request usually these are going to be a large spec or large task or feature you want built and then you're going to have validations all right validators and then give some direction with your resolution direction with your resolution all right request validate resolve that's the anatomy of these closed loop prompts that's the structure of these closed loop prompts that allow your agents to test themselves. So let's scale this up. Right. We're missing a couple of things. All right. In particular, we're missing some front end tests. All right. We're also missing more importantly, end to end tests. We want our agents testing the application. I want our agents to be able to fully test this application, no matter how complex like you or I would. Right. Let's scale would. Right. Let's scale it up again. all around slash clear just to keep the agents context window fresh. And now we're gonna run this prompt. So let me paste this and let's break this down a little bit. So we've now turned this into a full reusable prompt that our agent can run, okay? This has gone from a toy prompt, right? A toy high level prompt to now a mid-level reusable prompt that you would want to embed in your codebase. All right, so how does this work? What does this look like? We have the purpose of the top validation, validate your work. You can see our same backend checks. I'm running a few front end validation checks here. If validation checks here. If you have some type of front end VITES or Jest tests, this would be the place to run those. That'll increase the confidence you have that your front end codebase is correct. And it'll give your agents the ability to correct the issues as they come up. And then we have something really cool here, something brand new. So I'm gonna just copy all this and let's go ahead and run this. And then we'll talk about what exactly is happening here. So let's fire this off. Our agent's gonna run top to bottom. You can see I have a couple instructions here. Run in order top to bottom. If you want to take any issues, stop and resolve them immediately. Then rerun them immediately. Then rerun every validation step, okay? It's not good enough. to resolve this once and that resolution causes something to break, right? You need them all to run from start to finish. What's happening down here. We are running the Playwright MCP server. Of course, valuable links is going to be available in your loot box, detailing all of these services, detailing all the tech we use in this lesson. We're running the Playwright MCP server to run the simple validation workflow. So we're going to open the browser. We're going to take a screenshot. We're gonna run screenshot. We're gonna run a query. Our agent is gonna type for us, right? It's gonna operate the browser, query, take a screenshot, and then read both images and verify that the natural language query was executed correctly, all right? confirm seven results are returned. So this is a very specific test, right? It validates a precise result from our codebase given the state of our code base. All right. So you can see it here working through. If we scroll back up, you can see those tests are passing, right? The codebase is in a good state right now. So that's great. That's a CD into the right directories. into the right directories. That's totally cool. Nothing interesting happening there. TSC emit. So we have no front end TypeScript issues. Okay. So once again, We've increased the confidence just a little bit. And then we're going to compile our front end application here. There it is, Bunrun build. Our front end is now built for production. Again, we're stacking the confidence. We know a little bit more, a little bit better that things are going to work. So here we go. Now Playwright is executing. Check this out. If you haven't played with this yet, if you're not aware of this, this is huge. Browser control is huge. Browser control is another tool that you can use to validate work. And keep in mind, I'm passing in a local URL. When you're building these systems, when you're setting up your ADW to test your system and to validate your work, you might point this at a staging system or even a production test account so that you know that the work is complete, right? And notice what every one of these things are doing for us. We're giving our agents the ability to validate that the work is complete, okay? This helps us okay? This helps us stay out the loop, right? This is literally happening on this machine right now. We fired off an entire feature to get built. This is happening on this device, all right? And we're building up to that. Let's go ahead and see how we've done here. It's got the screenshot end-to-end after, and now it's reading those files. And let's see what it's done with the validation. It looks like everything's working. It's not going back in and building anything. Let's see what it's done here. And you can see here, you know, I've removed the request part of the closed loop. This is just validation and resolution. I just wanna focus I just wanna focus in here on what an isolated agent that's just testing will look like. And so check this out, right? We have two screenshots saved. We go to refresh terminal here, type GS for Git status. You can see we have those modified files from our linting executions and we have our before and end files. So if we look at before, you can see a real snapshot of our codebase. And you'll notice, you know, we have a couple of modifications here from the original state. It looks like our tests are modifying our code base here, which is an issue. Bookmark that. We're gonna fix that in a second. But in a second. But you can see here, there's the before and let's look at the after. Check this out. So you can see here those exact results. There's no reason to be, you know, skeptical here. We can go app client end to end after. And let me just, you know, take a look at this. I can use this cool feature. and just copy this query exactly. Open up the browser, run this query, and I can see those exact results, all right? Fantastic. So, you right? Fantastic. So, you know, our agent is literally doing work that we would do to validate. And of course, I know, I know this application is super simple. It's minimal, right? It's very easy to prompt what exactly to do. You know, there's like five actions you can take on this page, but don't let that scare you away. Don't let that deter you. Let's be super, super clear about what's happening here. Our agent has more agency with self validating loops. Okay. If you ignore giving your agents testing, if You do not build You do not build in these closed loop systems with these powerful closed loop prompts. You are going to be wasting more time. Your agents can be solving issues if you just give them a little more feedback and then a little more and a little more until they'll be able to solve a massive, massive shot of issues, massive buckets of issues because they have feedback loops. And so the whole point of what we're doing here is save yourself some more time, become more of an Agentic Engineer, lean into Agentic Engineer, lean into the future. Don't lean into the present, don't lean into the past, lean into the future, okay? Hand off more responsibility to your agents, not less. We're increasing confidence, we're increasing the fact that we know our agent has shipped work for us with every test we add, with every validation. And you know, just to mention it again, you know, what do we have here? And we have one, two, three, four, five, and then six. closed loop tests and really, you know, pie test is a crap ton more than, crap ton more than, you know, just one, but we can put it under one bucket, right? You know, with this, we're adding tons of confidence to our agents. Now, let's move forward from this, right? We now know that we can add these powerful closed loop prompts where our agents can validate their own work. They can know that what they've done is correct, just like you or I can by running tests, by building, by compiling, by looking at the browser, right? We're handing this off to our agent. So how do we take this a step further? How do we embed this into the Agentic layer, right? The new Agentic layer that we're Agentic layer that we're building around all of our codebases. How do we teach the agents in our code base how to test? We can of course embed them into our templates, into our reusable prompts. So if we open up dot Claude, go into commands, you can see we have a growing list of reusable natural language solutions that our Agentic coding tools can use to ship to solve all types of problems that we run into in our codebase. Okay. For instance, we have a bug prompt. Let's a bug prompt. Let's dial into that and let's fix this issue, right? If I go here and I hit refresh, We're getting the new tables. Our test deleted a user table. They also added this one and they added this, you know, drop user. So we have a bug in the codebase. Okay. Let's resolve that. Of course, we're not coding anything, right? We've stopped coding. We know that that's a key tactic. It's the first tactic of Agentic coding. What we're doing here is adding feedback loops in to our templates. Okay. So once again, we're templating our engineering. If we our engineering. If we search end to end in this bug file, let me close these other files. And let me actually just, Where are we here? Let me revert the code base, clean everything up. Let's remove these two files and let's proceed. All right, so just clean up the code base real quick there. So what are we gonna do here? We are going to search end to end. So check this out, important. If this bug affects UI or user interactions, we're gonna add a test. Let me backtrack a little bit, right? We have our bug reusable meta prompt. All right, so we covered right, so we covered this in our previous lessons. If we collapse everything here, you can see all of the sections and the key part is just the top. This is a meta prompt. It's a prompt that creates a prompt. We're templating our engineering into some key reusable prompts into the codebase so that we can solve problems very quickly like this. So let's hop back to that instance. Let's clear. We always want fresh agent instances. We don't want any context pollution and then I'll run slash bug. And let's look at the variables that this prompt variables that this prompt takes in. So we need an issue number. We'll just type one, one, one here. We need an ADW ID. This is running on its own. We're running in the loop. So there's no AI developer workflow ID. We'll just type ADW one, one, one. And then we need our actual prompt, right? The issue that gets passed in here. We need to resolve this issue. We have a couple of tests. that are creating some real junk data in our production database, right? We're running in SQL lite, we can run these in memory. Our tests shouldn't be operating in our quote unquote live production database. So we'll run this prompt. we'll run this prompt. Basically, I'm just describing this at a high level. Then our bug meta prompt is gonna pick this up, it's gonna create a plan for us, okay? What have we done here with this bug plan, right? Because we had this before, how have we improved this? So a couple things, you may have noticed even before in our previous lesson, If you look at the plan format, the template that our agent is filling out for us, if we scroll down here, you can see we have validation commands and you'll notice something here. It's templated in to the plans that get generated by our agent. We by our agent. We have validation commands, right? So with every plan we build, with every bug we solve, with every feature we build, we are going to have validation commands baked in by default. Okay. When we talk about scaling our impact with Agentic coding, when we talk about always adding feedback loops, I mean it always add feedback loops and you can do this. You know, this isn't a huge cumbersome task. You encode it, you template your engineering into your reusable prompts so that your agents can pick up on the work and on the work and build the way you want to build over and over and over. And the way you want to build is with high confidence. You want to know that the feature has shipped. Okay. We also have another line in here, right? A template inside of the prompt. If you create an end to end test, include the following validation steps. So we're just saying, you know, specifically add this in addition to all the other validation commands that you'll use to validate with 100% confidence, right? We're just doing some prompting here that the issue has been fixed with no regressions. All right. So super powerful stuff here. You can see our agent is It's still running, it's looking for running, it's looking for this issue. It looks like it's gonna find it pretty soon here. While we're doing this, while this is running, we have an agent running here. We have an agent running here. Let's fire up another issue that I noticed that I wanted to fix in this lesson. When we run queries, so if I say select five products, the input field does not get disabled. So you could keep querying over and over, right? This is like a common UI thing. This is not something that you or I need to solve ever again. Right, even with just a even with just a simple in loop prompt, we could fire that off and it will resolve this. But we don't just wanna know that the issue is fixed, we wanna know that the issue is fixed and we want our agent to be confident about that. Okay, so we can fire off another agent here. and we can have our agent fix that input issue. So I'm just gonna copy this in here, bug, give it an ID ADW222, disable the input query when the query is running and add a debounce request, all right? So we can fire that off while our other agent is working for us. And let's go ahead and see if that test has completed. There we go. So it's finished. So this is our plan this is our plan that it got generated, right? So just to make it super clear here, specs, issue 111 ADW, and you can see the rest of the ID there. This is our plan step in the software development lifecycle. We've automated this and we've solved the problem of scaling our impact with our plans, right? Our plan velocity is through the roof now because we've templated our engineering. How does test plan of this? You can see here we have a great bug description that's detailing the exact issue, right? It expanded on the high level prompt that we sent in and turned it into a turned it into a detailed low level spec. Great planning is indeed great prompting. And we have a prompt to automate writing great plans for our codebase, right? So it's very specific. There's our reproduce steps. And you should be able to see right our agent has correctly identified that you should be able to just look at the codebase and look at the tables to see these error tables so our agent can of course just execute this you can see no new files and if we search end to end there isn't any hint of the these end to end files right this is files right this is a back-end specific change so our if statement here right to kick off our end to end playwright tests that did not get activated because our agent correctly identified the fact that this bug does not affect the UI. But so in the generated plan, we can search through this. And if we, you know, look, here's all the step-by-step play. But if we come down here to the bottom, the validation commands, you can see everything our agent will do to validate that the bug was fixed. All right, so we have a couple SQLite three runs. We have UVPyTest, right? Testing that UVPyTest, right? Testing that specific file that had the issue. We have diff before and after table. So it's actually writing to standard out before, after, then we diff it, okay? And so these are all validations, right? We've encoded the act of validating into our prompts. This is very, very powerful. When our agent inevitably makes a mistake, its validation command will create a closed loop, several closed loops, right? You can see, We're testing a multitude of things here. It'll create a closed feedback loop for closed feedback loop for our system to correct itself. All right, so let's look at how our other plan has done. You can see we got this input debounce plan. While we're waiting here, let me just go ahead, let's kick off the implementation of this. And so we can implement this with slash implement and we'll pass in this file. All right, and so that's all we need. Oh shoot, we have a overload. Let's run the implement again. So we've cleared and now we're going to implement. Hopefully this works. There we go. So that looks great. All right, so now our right, so now our plan is going to get implemented here. And if we open up that implement command, we've looked at this in previous videos. This is a higher order prompt that takes a plan in as an argument and then has a couple instructions around the plan. All right. So that's that this engineering resource is now going to get kicked off and create the solution for us. So let's look at our input debounce. And what I want to point out here is the difference between these two plans, right? Because we've template or engineering and we've added feedback loops into the templates. We can the templates. We can scroll down. We can see we have this new file. in our solution for our disable input debounce query. All right, so when we make a query, we want to disable the input field so you can't spam it, right? This is a classic front end problem. This is great. So we have this end to end test that's gonna get built here as a new file. Everything else is just changes to existing files, okay? And if we scroll down here, one of the tasks is create end to end test, right? So it's saying read this file and this file. and then create an end-to-end test. And we're gonna dig into what the end-to-end into what the end-to-end tests look like, but it's very similar to our Playwright prompt that we looked at before, right? Our closed loop prompt, you can see we have clean user store here that describes what we're testing exactly. test steps we have this powerful information dense keyword verify we have a variable here that we're referencing from a higher order prompt and then we have our success criteria these things must be true in order for this test to pass and then we'll dive into the test end to end later on in this lesson but you can see here this test thanks to us templating thanks to us templating our engineering practice of testing into our bug reusable meta prompt this is now agentically going to create this end-to-end test for us. And we can of course spin this up as well. I'm gonna copy the reference to this, go to this terminal, I'm gonna clear it, right? Even though this will probably be somewhat useful to have the context of it being built, always run fresh agents is a good practice and it moves you toward true off device Agentic coding where you're gonna have multiple agents picking up context, picking up work from picking up work from previous agents with fresh focus context windows, right? So we'll type clear implement and I'll pass in this plan and let's go ahead and get to work on that. And actually I'm gonna hold off on this. I forgot I'm running in the loop. I'm running in loop Agentic coding. So this might modify files. This agent is kind of operating my device. We're not running on Git work trees or anything, right? Unlike my isolated device here that can do whatever it wants, right? We can run agent can take over the device here. I the device here. I have to run one at a time, okay? And this is one limitation of in loop Agentic coding. You're pretty much limited one agent unless you're conscious about everything that you're changing. Okay. This is great. This is running. It looks like we're going to implement that. This is going to build up the future. I'm not super concerned with the results of this. I mainly just want to showcase how our agent is going to run these self-validating loops, right? So inside of this plan, right in our issue one, one, one, or fixing this validation command, right? You can see our agent has started the test has started the test to validate step. So it's going to first run tests. and then it's gonna execute all of our regression validation tests, right? So it's gonna run all of these commands next. And so this is super powerful. This is gonna keep working and you can see the closed loop system getting to work, okay? I can see that there are some tables in the database. I'm running thinking mode by the way with the information dense keyword in our implement higher order prompt, right? A prompt that takes a prompt as a parameter, right? An argument. You can see I have think hard. the have think hard. the Claude Code, Anthropic, Encoded Information Dense Keyword. This is a principle of AI coding. There are keywords that you can create or that you'll be able to reference that have more meaning, that have some special effects. The Think Hard activates the Cloud Series models thinking modes, right? So you can see that happening here. Elec already knew that, but it's important to mention. We are now running tests and we're executing our validation commands. So if you open up the validation commands, you can see all these tests are running. They're passing. That looks great. And this agent is going to run to going to run to completion here. Let's go ahead and move on to the next abstraction level. We've You know, started with our simple closed loop prompt. We're just running a linter. We then added entire backend tests with PyTests and we're compiling the backend now. We're letting our agent know that it's done the job to completion. And then we added full on backend, frontend, end to end tests. Again, we're just stacking up. We're increasing the confidence that our agent has done everything to completion without regression. We've then templated our engineering. And so our engineering. And so our slash bug command here, if I open up a new instance here, right? If I type slash bug, I just have access to this template and I can just quickly, you know, solve any type of bug inside of my codebase, right? With just one slash command, very powerful. We're scaling up. We're letting our agents do more work. And most importantly, we can quickly run all this work in an off loop device with our ADW tooling, right? With our AI developer workflows, which we'll get to our big last workflow here in just a moment. We then templated our engineering. then templated our engineering. So what comes next? after this, right? After you have a reusable prompt, that's a meta prompt, right? That generates consistent results for you. You then wanna get off your device, right? We need to get out the loop. It's a key tactic of Agentic coding, stay out the loop. That's where the ADWs come in, all right? So you can see here, lesson by lesson, we're improving the Agentic layer around our codebase. And, you know, let's highlight the files that really make that up, right? The directories, that make The directories, that make up the Agentic layer of our codebase. All right, let's highlight those here. And the most important one, right, the highest leverage point, the highest composition level is of course the AI developer workflow. And so we have this ADW test workflow here. This is going to run a series of prompts. And let's go ahead and point out the specific ones that it runs. So our ADW test does this. So it fetches a GitHub issue. Right. We want to run this from outside the loop using the Peter framework. We have four elements of Outlook systems, prompt of Outlook systems, prompt input, trigger environment and review system. And you can see with this ADW, we're tapping into every one of those. All right. So we kick this off like this. We're going to fetch the issue. We're going to run the application test suite. We're going to report the results. We're going to commit the results and then push NPR. So this workflow quite literally test the application and fixes any issues. Okay. And this is very powerful, right? It's completely separate from the build and plan step. If you remember the plan step creates our spec slash tour or it'll run slash feature or run slash feature or slash bug. And so that's the plan step. The build then takes the plan and actually implements. Okay, right. So this runs our slash implement with a path to our plan. And then ADW test does a few things. It runs a different type of Agentic workflow. Let's go ahead and collapse and let's go ahead and walk through the key elements of this. just at a high level. Let's walk through the main of the AI developer workflow for testing. Of course, you can run this on your device. It'll run the entire workflow. But the entire workflow. But the big idea, right, we are Agentic coding and we want to get more work into our agents hands. The idea is that we'll run this off device with a prompt we kicked off at the beginning of the lesson, right? You can see here, we take a GitHub issue number and an ADW ID so that we can chain together these AI developer workflows. We do some setup. We set up our new state variable. We get the repo, right? And then it's just a bunch of setup work here, but the real work comes here. here, when we start running our test suite, we're going to run tests with resolution. And so if we click into this, we click into this, we have this high level method that's going to run tests. And so it's going to run all these tests a number of times. And if we go into run tests, everything comes down to a reusable prompt, right? As cloud code calls these a custom slash command run test does this, let's go ahead and open this up, right slash test MD purpose here. So run front end back in tests and return a standardized JSON format so that we can automate agents. All right, so let's skip over some of the instructions. You can of course dial into this codebase, fully understand codebase, fully understand it, copy it, make it your own. But the key pieces are here, right? Front end test, back end test, and then report in this format. Okay, so we're running our back end tests, just like you saw before. A couple more details, nothing, you know, massively changed here. All right, we're compiling some files, running rough, running PyTest. All right, same thing for the front, TypeScript compile. and then we're building the front end, right? We're adding more validation layers. Great. We're then reporting in this JSON structure. Why are we reporting in this JSON structure? Well, so that we can hand off we can hand off any issues that arise to individual agents, right? By getting concrete feedback, we can pass these into full on new isolated agents with their own context window. I hope you can see how things are starting to work together. I hope you're seeing some common patterns. You can do anything you want in the ADW. This is where you compose your prompts, your Agentic prompts, the new world of engineering with the old world, right? Just deterministic code. After we run these tests, right, we have our test responses, we parse the responses, we parse the results, and then we try to resolve any failed tests. And what do you think the failed tests do? Of course, they do one thing. They run a template, resolve failed test. resolve failed test, right? So we're going to get that failed test input that was created by our top level testing agent. And then we're going to fire off a new individual agent right here. Okay, you know, we're looping through all the failed tests and kicking off individual agents to resolve the failed test. Okay, so this is super important. We can parallelize important. We can parallelize this, we can do a lot of things here with the architecture of our ADW. The key here is we have a mechanism inside the codebase to run our tests to run every test we have. And then to resolve the issues, right? And it's isolated. It's not part of the build. It's not part of the plan. This runs on its own with its own set of agents in its own context windows. Okay. And so this is why it's so important to adopt our agent's perspective. We want to know what's inside of every agent at any time. And so we have one more key piece of this, of piece of this, of course, if we scroll up to that test end to end, so we have our end to end tests, and this is gonna run our playwright tests. We can search slash test end to end. If you're ever wondering where a prompt is in the codebase, just type slash, and then, you know, the name of the file and you'll find it. Let's break down our test end to end. You can see that this is indeed a higher order prompt. This is a hop because we're passing in another prompt that is the end to end test file. And those are all stored here. So you can see here, we just have a few end to end test files and it's gonna do exactly what we do exactly what we did in the beginning, right? Simple test here. It's just gonna walk through individual steps in play, right? It's gonna confirm results, verify things and our top level end to end test prompt takes in a slew of variables. It has instructions. We're setting some things up. So we have some pre conditioned commands we want to run, right? Reset DB, make sure everything's running. And then we have, you know, screenshot directory, some details about this port and then the output formats. You the output formats. You can see exactly what this is reporting for every end to end test that runs. All right. So that's this and we're looping through here. We're running on every one of our end to end test files. As you can see, right, I know I'm kind of moving through this quickly, but there's a lot here. Really take the time, dial into this file, understand what's going on. But the key here is it's actually quite simple. We're taking all of our end to end tests that we're going to be building up as we stack up the work we're doing, the useful work we're doing for users in this codebase. in this codebase. And then we're going to be able to quickly test them end to end. All right. So this is a way to scale what your agents can do, it's a way to scale the work is tested and you'll notice it's gonna run every single test, it's gonna run every single test and then try to resolve any issues agentically. So this is our run end to end tests. that's in run end to end test with resolution. So we're gonna loop through end times and try to correct any issues if there are any. It's all controllable with our retry parameters here at the top. And you the top. And you can see several different agents that run in this workflow. So, you know, previously we looked at our plan and build step. We now have, you know, multiple different files that we can run and compose together so that our loops are isolated. And this is powerful because we want to be able to tap in to one node of the software developer lifecycle and improve that one node. So we have plan, we have build, we have test, and of course we have the compositions. So there's ADW plan and here's ADW plan. build, right? So in this workflow, we just fire off we just fire off the scripts back to back. This is again, the importance of having isolated reusable scripts. We're using astral UV to accomplish this. You can use anything you want, classic shell scripts, bun scripts, whatever you wanna do, but you just need to make sure that these are isolated in their own, you know, separate Agentic layer in your codebase, right? And this is what the ADW directory does for us. Of course, you can see a couple of different modules. We've broken things down a little bit more. We've added ADW tests, right? So while we're building this out, we of course have some tests. We're getting really meta now. Ultimately, really meta now. Ultimately, it all builds up to this, right? ADW plan build test. And this is what ran in the beginning. Let's go ahead. You can see we have, you know, the exact steps built up there. We are working our way through the software development lifecycle with our agents. And if we open up the browser here, we can check on that feature. So inside this issue, you can see we have a PR attached and we have a bunch of work recorded. And you can see this workflow took some time and it ended up with one test failure. And one test failure. And it looks like, so we got a lot of great tests completed. Test complex query. We got a Claude Code error. This is probably a API error. Okay, so this is a Claude Code internal error, which of course will break the tests. Everything needs to run on Claude Code here. And if we want to, we can always dig into the logs to validate. I'll bet this was a overloaded API error on the Anthropic side. Let me just go ahead and confirm that. We can dig into the logs here. This will be a good exercise as well. Let's go into our agent box. and let's look up that exact ADW ID right here. ADW ID right here. We can see this at the top of the file here, starting workflow ID. This is our AI developer workflow ID. We can copy that and then we can quickly search that. And this is the importance of observability and tracing. You wanna know exactly what's going on inside of our code base. In these code bases, we have this agent directory as well as our logs directory that trails and traces everything that's happened. We're gonna be talking about documentation and reviewing and subsequent lessons, but let subsequent lessons, but let me just quickly look through this in our agent sandbox here. I just wanna confirm exactly what this error was here. Claude Code error, test execution error. We can hop into the raw output and we can see exactly what happened here. So if we trace this log right here, end to end, we had a test resolver run and you can check this out, API error 500. So this is always good to have, right? You wanna be able to trace everything right down to the nose. You can see in this ADW ID inside this ADW ID inside this log director, we have all of our agents run. We can look at and inspect any agent that ran and we can see in this file, it looks like it was our end to end test resolver iteration one. It ran this prompt, tried to resolve cloud code failing, which it just can't. You know, we can see we have API error 500 right here. So anyway, jumping out of the sandbox, we can see we still made a lot of progress here. We got all the way to the bottom and we have a PR. So we can go ahead, take a look at this PR. You can see it detailed exactly what we detailed exactly what we wanted, natural language query button that generates interesting queries. we can look at the files generated. You can see it created a new end-to-end test for us. And we have file updates, that looks great. We have our spec, so we're gonna review exactly the plan that was created, right? So you can see the end-to-end test is detailed here. And so if we open up our machine here, our dedicated agent environment, we can actually do something really cool, right? It's operating this codebase is operating this environment all on this environment all on its own. So if we open up the application, you can see we have that new feature generate random query. If we click this, we have the work completed, right? You see the loading state has the button a little off to some additional testing steps would be good here to find this, but it's doing the work. We can hit query. Let's go ahead and see what this generates for us. And nice, so it generated quite a complex query there to give this result. Obviously we have test data here, so it's not gonna be super sharp, but we can generate random queries over and over queries over and over and over. This is pretty cool, all right? And this all happened off device with our powerful Outloop system that built, planned and tested. And so now you and I, the human engineer comes in here at the end and makes sure that everything looks good. It's the test on top of the test, right? This is higher confidence. This is improved engineering. More and more, you can see we're shipping end to end. There are a lot of key big ideas here. We are generating plans with are generating plans with Meta prompts. We've embedded testing into our daily work, right, into our bug fixes, into our features, into our chores. But then we pushed it even further. into ADWs, right? So we're composing the 12 leverage points of Agentic coding step by step by step. Even after you have ADWs, you can stack them together, of course, to complete the software developer lifecycle and you can run them in isolation to improve them, right? So what's next? right? We're doing a lot here. We're moving fast. We're building moving fast. We're building up this powerful Agentic layer around our code base. There's a lot going on here. In lesson six, we conclude our epic journey to automate this software developer lifecycle. We are almost there. We need two more key steps. We need the review step and the document step. Along the way, we're going to clarify and really walk through what a complete Agentic layer around our codebase looks like. We have all the foundations. We have all the big heavy hitting ideas. from high level prompts, low level prompts, plans, templates, level prompts, plans, templates, meta prompts, scaling all the way up to ADWs where we compose workflows and get them off our device. We know about the Peter framework we can use to run Outloop Agentic coding systems. And of course, whenever we need to, we can hop inside the loop, go hands on and resolve things very quickly. You can see here this bug on our main branch right on this device was fixed. It reran all of our tests and it made sure that we don't have this problematic issue here. So we can here. So we can come in here, refresh, and that table is no longer showing up. Everything's looking great here. That was resolved. Our agents are doing more and more for us. So successful products grow, they become complex. We need agents to keep track of everything as they begin to operate our codebase. In lesson six, we conclude our epic journey along the software developer lifecycle where our agents will nearly be operating our entire codebase themselves. And so I know this is a lot. We're moving quickly along this software developer lifecycle. this software developer lifecycle. We need to pack this in so we can get to some big key Tactical Agentic coding in our advanced lessons along lessons six, seven, and eight. There's lots to say again, we glossed over some details, but it's all here in the code base. Definitely check this out, spend some time here, understand what the Agentic layer of your codebase can look like. Start building these validation loops into your codebase, stand up concrete AI developer workflows. They don't have to be complex or intricate. be complex or intricate. Everything starts with a single prompt. All right. And everything starts in the loop and then slowly move it out the loop. Get yourself a dedicated environment so that you can operate out the loop. We're rolling. We're making progress. I'll see you in lesson six where we close the loop on the software developer lifecycle. There we're going to teach our agents how to not just build, plan and test, net new features and fixes to your codebase, but we're going to show them how to review and document. Code bases grow. They evolve. bases grow. They evolve. As we add features, we need to update documentation. And as we ship, we need to review the work completely. We've already taken a big step into that. In the next lesson, we continue it and we automate the software developer lifecycle end to end. We're giving our agents full control over the codebase. Great work here. Take some time to digest these ideas and I'll see you in Lesson 6.